#+TITLE: Notes on: Sequential and parallel algorithms and data structures by Sanders, P., Mehlhorn, K., Dietzfelbinger, M., & Dementiev, R. (2019)
#+Time-stamp: <2021-06-05 14:30:26 boxx>

- source :: cite:sanders-2019-sequen-paral

* TODO Summary

An algorithm is a precise and unambiguous recipe for solving a class of problems. If formulated as programs, they can be executed by a machine. Algorithms are at the heart of every nontrivial computer application. Therefore, every computer scientist and every professional programmer should know about the basic algorithmic toolbox: structures that allow efficient organization and retrieval of data, key algorithms for problems on graphs, and generic techniques for modeling, understanding, and solving algorithmic problems.

* TODO Comments

* TODO Topics

** Asymptotic Notation

The main purpose of algorithm analysis is to give performance guarantees, for example bounds on running time, that are at the same time accurate, concise, general, and easy to understand. It is difficult to meet all these criteria simultaneously. For example, the most accurate way to characterize the running time /T/ of an algorithm is to view /T/ as a mapping from the set /I/ of all inputs to the set of nonnegative numbers \R_{+}. For any problem instance /i/, T(i) is the running time on /i/. This level of detail is so overwhelming that we could no possibly derive a theory about it. A useful theory needs a more global view of the performance of an algorithm.

Hence, we group the set of all inputs into classes of "similar" inputs and summarize the performance on /all/ instances in the same class in a single number. The most useful grouping is by /size/. Usually there is a natural way to assign a size to each problem instance. *The size of an integer is the number of digits in its representation, and the size of a set is the number of elements in that set*. The size of an instance is /always/ a natural number. Sometimes we use more than one parameter to measure the size of an instance; for example, it is customary to measure the size of a graph by its number of nodes and its number of edges. We ignore this complication for now. We use size(i) to denote the size of instance /i/, and I_{n} todenote the set of instances of size /n/ for /n/ \in \N. For the inputs of size /n/, we are interested in the maximum, minimum, and average execution times.

 - worst case:     T(n) = max{T(i) : i \in I_{n}}
 - best case:      T(n) = min{T(i) : i \in I_{n}}
 - average case:   T(n) = \cfrac{1}{|I_{n}|}\sum T(i)

We are most interested in the worst-case execution time, since it gives us the strongest performance guarantee. A comparison of the best and the worst case tells us how much the execution time varies for different inputs in the same class *(!)*. If the discrepancy is big, the average case may give more insight into the true performance of the algorithm.

We shall perform one more step of data reduction: We shall concentrate on /growth rate/ or /asymptotic analysis/. Functions f(n) and g(n) have the /same growth rate/ if there are positive constants /c/ and /d/ such that /c/ \le f(n)/g(n) \le d for all sufficiently large /n/, and f(n) /grows faster/ thatn g(n) if, for all positive constants /c/, we have f(n) \ge c \cdot g(n) for all sufficiently large /n/. The growth rate talks about the behavior for large /n/. The word "asymptotic" in "asymptotic analysis" also stresses the fact that we are interested in the behavior for large /n/.

Why are we interested only in the growth rates and the behavior for large /n/? We are interested in the behavior for large /n/ because the whole purpose of designing efficient algorithms is to be able to solve for large instances. For large /n/, an algorithm whose running time has a smaller growth rate than the running time of another algorithm will be superior. Also, our machine model is an abstraction of real machines and hence can predict actual running times only up to a constant factor. A pleasing side effect of concentrating on growth rate is that we can characterize the running times of algorithms by simple functions.

The following definitions allow us to argue precisely about /asymptotic behavior/. Let f(n) and g(n) denote functions that map nonnegative integers to nonnegative real numbers:

 - O(f(n)) = {g(n) : \exists c \gt 0 : \exists n_{0} \in \N_{+} : \forall n \ge n_{0} : g(n) \le c \cdot f(n)},
 - \Omega(f(n)) = {g(n) : \exists c \gt 0 : \exists n_{0} \in \N_{+} : \forall n \ge n_{0} : g(n) \ge c \cdot f(n)},
 - \Theta(f(n)) = O(f(n)) \cap \Omega(f(n)),
 - o(f(n)) = {g(n) : \forall c \gt 0 : \exists n_{0} \in \N_{+} : \forall n \ge n_{0} : g(n) \le c \cdot f(n)},
 - \omega(f(n)) = {g(n) : \forall c \gt 0 : \exists n_{0} \in \N_{+} : \forall n \ge n_{0} : g(n) \ge c \cdot f(n)}.

The left-hand sides should read as "big O of /f/", "big omega of /f/", "theta of /f/", "little o of /f/", and "little omega of /f/", respectively. A remark about notation is in order here. In the definitions above, we use "f(n)" and "g(n)" with two different meanings. In "O(f(n))" and "{g(n) : ...}", they denote the functions /f/ and /g/ and the "/n/" emphasizes that these are functions of the argument /n/, and in "g(n) \le c \cdot f(n)", they denote the values of the functions at the argument /n/ *(!)*.

The growth rate of most algorithms discussed in this book is either a polynomial or a logarithmic function, or the product of a polynomial and a logarithmic function.

Asymptotic notation is used a lot in algorithm analysis, and it is convenient to stretch mathematical notation a little in order to allow sets of functions (such as O(n^{2})) to be treated similarly to ordinary functions. In particular, we shall always write /h/ = O(f) instead of /h/ \in O(f), and O(h) = O(f) instead of O(h) \subset O(f). For example,

\[ 3n^{2} + 7n = O(n^{2}_{}^{}) = O(n^{3}) \].

Never forget *(!)* that sequences of "equalities" involving /O/-notation are really membership and inclusion relations, and as such, *can only be read from left to right*.

If /h/ is a function, /F/ and /G/ are sets of functions, and \circ is an operator such as +, \cdot, or /, then /F/ \circ /G/ is a shorthand notation for {f \circ g : f \in F, g \in G}, and /h/ \circ /F/ stands for {/h/} \circ /F/. So f(n) + o(f(n)) denotes the set of all functions f(n) + g(n) where g(n) grows strictly more slowly than f(n), i.e., the ratio (f(n) + g(n))/f(n) goes to 1 as /n/ goes to infinity. Equivalently, we can write (1 + o(1))f(n). We use this notation whenever we care about the constant in the leading terms but want to ignore /lower-order terms/.

The following rules hold for /O/-notation:

 - \( cf(n) = \Theta(f(n)) \) for any positive constant c,
 - \( f(n) + g(n) = \Omega(f(n)) \),
 - \( f(n) + g(n) = O(f(n)) \) if \( g(n) = O(f(n)) \),
 - \( O(f(n)) \cdot O(g(n)) = O(f(n) \cdot g(n)) \).

** The Sequential Machine Model

In 1945, John von Neumann introduced a computer architecture which was simple, yet powerful. The limited hardware technology of the time forced him to come up with a design that concentrated on the essentials; otherwise, realization would have been impossible. Hardware technology has developed tremendously since 1945. However, the programming model resulting from von Neumann's design is so elegant and powerful that it is still the basis for most of modern programming. Usually, programs written with von Neumann's model in mind work well on the vastly more complex hardware of today's machines.

The variant of von Neumann's model used in algorithmic analysis is called the *RAM model*. It was introduced by Shepherdson and Sturgis in 1963. It is a /sequential/ machine with uniform memory, i.e., there is a single processing unit, and all memory accesses take the same amount of time. The (main) memory, or *store*, consists of infinitely many cells S[0], S[1], ...; at any point in time, only a finite number of them will be in use. In addition to the main memory, there are a small number of *registers* R_{1}, ..., R_{k}.

The memory cells store "small" integers, also called *words*. In our discussion of integer arithmetic in Chapter 1, we assumed that "small" meant one-digit. It is more reasonable and convenient to assume that the interpretation of "small" depends on the size of the input *(!)*. Our default assumption is that integers whose absolute value is bounded by a polynomial in the size of the input can be stored in a single cell. Such integers can be represented by a number of bits that is logarithmic in the size of the input. This assumption is reasonable because we could always spread out the contents of a single cell over logarithmically many cells with a logarithmic overhead in time & space and obtain constant-size cells. The assumption is convenient because we want to be able to store array indices in a single cell. The assumption is necessary because allowing cells to store arbitrary numbers would lead to absurdly overoptimistic algorithms. For example, by repeated squaring, we could generate a number with 2^{n} bits in /n/ steps. Namely, if we start with the number 2 = 2^{1}, squaring it once gives 4 = 2^{2} = 2^2^^1, ..., etc.

Our model supports a limited form of parallelism. We can perform simple operations on a logarithmic number of bits in constant time.

A RAM can execute (machine) programs. *A program is simply a sequence of machine instructions, numbered from 0 to some number /l/.* The elements of the sequence are called *program lines*. The program is stored in a program store. Our RAM supports the following *machine instructions*:

 - R_{i} := S[R_{j}] /loads/ the contents of the memory cell indexed by the contents of R_{j} into register R_{i.}

 - S[R_{j}] := R_{i} /stores/ the contents of register R_{i} in the memory cell indexed by the contents of R_{j}.

 - R_{i} := R_{j} \odot R_{h} executes the binary operation \odot on the contents of registers R_{j} and R_{h} and stores the result in register R_{i}. Here, \odot is a placeholder for a variety of operations. The /arithmetic/ operations are the usual +, -, and \*; they interpret the contents of the registers as integers *(!)*. The operations *div* and *mod* stand for integer division and the remainder, respectively. The /comparison/ operators \le, \lt, \gt, and \ge for integers return /truth values/, i.e., either /true/ ( = 1) or /false/ ( = 0). The /logical operators/ \land and \lor manipulate the truth values 0 and 1. We also have bitwise Boolean operations | (OR), & (AND), and \oplus (exclusive OR, XOR). They interpret contents as bit strings *(!)*. The shift operators >> (shift right) and << (shift left) interpret the first argument as a bit string *(!)* and the second argument as a nonnegative integer *(!)*. We may also assume that there are operations which interpret the bits stored in a register as a floating-point number, i.e., a finite-precision approximation of a real number.

 - R_{i} := \odot R_{j} executes the /unary/ operation \odot on the contents of register R_{j} and stores the result in register R_{i}. The operators -, \neg (logical NOT), and \~ (bitwise NOT) are available.

 - R_{i} := C assigns the /constnat/ value C to R_{i}.

 - ~JZ~ /k/,R_{i} continues execution at program line /k/, if register R_{i} is 0, and at the next program line otherwise (/conditional branch/). There is also the variant ~JZ~ R_{j},R_{i}, where the target of the jump is the program line stored in R_{j}.

 - ~J~ /k/ continues execution at program line /k/ (/unconditional branch/). Similarly to ~JZ~, the program line can also be specified by the content of a register.


A program is executed on a given input step by step. The input for a computation is stored in memory cells S[1] to S[R_l] and execution starts with program line 1. With the exception of the branch instructions ~JZ~ and ~J~, the next instruction to be executed is always the instruction in the next program line. The execution of a program terminates if a program line is to be executed whose number is outside the range 1 .. /l/. Recall that /l/ is the number of the last program line *(!)*.

We define the execution time of a program on an input in the most simple way:

#+begin_quote
Each instruction takes one time step to execute. The total execution time of a program is the number of instructions executed.
#+end_quote

It is important to remember that the RAM model is an abstraction *(!)*. One should not confuse it with physically existing machines. In particular, real machines have a finite memory and a fixed number of bits per register (e.g., 32 or 64). In constrast *(!)*, the word size and memory of a RAM scale with input size. /This can be viewed as an abstraction of the historical development/. Microprocessors have had words of 4, 8, 16, and 32 bits in succession, and now often have 64-bit words. Words of 64 bits can index a memory of size 2^{64}. Thus, at current prices, memory size is limited by cost and *not* by physical limitations. This statement was also true when 32-bit words were introduced.

Our complexity model is a gross oversimplification: Modern processors attempt to execute many instructions in parallel. How well they succeed depends on factors such as data dependencies between successive operations. As a consequence, an operation does *not* have a fixed cost *(!)*. This effect is particularly pronounced for memory accesses *(!)*. The worst-case time for a memory access to the main memory can be hundreds of times higher than the best-case time. The reason is that modern processors attempt to keep frequently used data in *caches* - small, fast memories close to the processors. How well caches work depends on a lot on their architecture, the program, and the particular input. Appendix B discusses hardware architecture in more detail *(!)*.

We could attempt to introduce a very accurate cost model, but this would miss the point. We would end up with a complex model that would be difficult to handle. Even a successful complexity analysis would lead to a monstrous formula depending on many parameters that change with every new processor generation. Although such a formula would contain detailed information, the very complexity of the formula would render it useless. /We therefore go to the other extreme and eliminate all model parameters by assuming that each instruction takes exeactly one unit of time/. The result is that constant factors in our model are quite meaningless - one more reason to stick to asymptotic analysis most of the time. We compensate for this drawback by providing implementation notes, in which we discuss implementation choices and shortcomings of the model. Two (2) important shortcomings of the RAM model, namely the lack of a memory hierarchy and the limited parallelism, are discussed in the next two (2) subsections.

*** External Memory

The organization of the memory is a major difference between a RAM and a real machine: a uniform flat memory in a RAM and a complex memory hierarchy in a real machine. In sections 5.12, 6.3, 7.7, and 11.5 we shall discuss algorithms that have been specifically designed for huge data sets which have to be stored in slow memory, such as disks. We shall use the /external-memory model/ to study these algorithms.

The external-memory model is like the RAM model except that the fast memory is limited to /M/ words. Additionally, there is an external memory with unlimited size. There are special I/O operations, which transfer /B/ consecutive words between slow and fast memory. The reason for transferring a block of /B/ words instead of a single word is that the memory access time is large for a slow memory in comparison with the transfer time for a single word. The value of /B/ is chosen such that the transfer time for /B/ words is approximately equal to the access time.

** Pseudocode

Our RAM model is an abstraction and simplification of the machine programs executed on microprocessors. The purpose of the model is to provide a precise definition of running time. Rather than actual code, we formulate our algorithms in /pseudocode/, which is an abstraction and simplification of imperative programming languages such as C, C++, Java, C#, Rust, Swift, Python, and Pascal, combined with the liberal use of mathematical notation.

We now describe the conventions used in this book, and derive a timing model for pseudocode programs. The timing model is quite simple:

#+begin_quote
Basic pseudocode instructions take constant time, and procedure and function calls take constant time plus the time to execute their body.
#+end_quote

We justify the timing model by outlining how pseudocode can be translated into equivalent RAM code *(!)*. We do this only to the extent necessary for understanding the timing model. There is no need to worry about compiler optimization techniques, since constant factors are ignored in asymptotic analysis anyway. The syntax of our pseudocode is akin to that of Pascal, because we find this notation typographically nicer for a book than the more widely known syntax of C and its descendants C++ and Java.

*** Variables and Elementary Data Types

A /variable declaration/ "/v/ = /x/ : /T/" introduces a variable /v/ of type /T/ and initializes it to the value /x/. For example, "/answer/ = 42 : \N" introduces a variable /answer/ assuming nonnegative integer values and initializes it to the value of 42. When the type of a variable is clear from the context, we shall sometimes omit it from the declaration. *A type is either a basic type (e.g., integer, Boolean value or pointer) or a composite type*. We have predefined composite types such as arrays, and application-specific classes. When the type of a variable is irrelevant to the discussion, we use the unspecified type /Element/ as a placeholder for an arbitrary type. We take the liberty of extending numeric types by the values -\infty and \infty whenever this is convenient. Similarly, we sometimes extend types by an undefined value (denoted by the symbol \perp), which we assume to be distinguishable from any "proper" element of the type /T/. In particular, for pointer types it is useful to have an undefined value. In the values of the pointer type "*Pointer to* /T/" are handles to objects of type /T/. In the RAM model, this is the index of the first cell in a region of storage holding an object of type /T/.

A declaration "/a/ : /Array/ [i..j] *of* /T/" introduces an /array a/ consisting of /j - i/ + 1 /elements/ of type /T/, stored in a[i], a[i+1], ..., a[j]. Arrays are implemented as contiguous pieces of memory. To find an element a[k], it suffices to know the starting address of /a/ and the size of an object of type /T/. For example, if register R_{a} stores the starting address of an array a[0..k], the elements have unit size, and R_{i} contains the integer 42, the instruction sequence "R_{1} := R_{a} + R_{i}; R_{2} := S[R_{1}]" loads a[42] into register R_{2}. The size of an array is fixed at the time of declaration *(!)*; such arrays are called *static*. In a later section, we show how to implement /unbounded arrays/ that can grow and shrink during execution.

A declaration "/c/ : *Class* /age/ : \N, /income/ : \N *end*" introduces a variable /c/ whose values are pairs of integers. The components of /c/ are denoted by /c.age/ and /c.income/. For a variable /c/, *addressof* /c/ returns a handle to /c/, i.e., the address of /c/. If /p/ is an appropriate pointer type, /p/ := *addressof* /c/ stores a handle to /c/ in /p/ and \*p gives us back /c/. The fields of /c/ can be then also be accessed through /p/ \rightarrow /age/ and /p/ \rightarrow /income/. Alternatively, one may write (but nobody ever does *(!)*) (\*p).age and (\*p).income.

Arrays and objects referenced by pointers can be allocated and deallocated by the commands *allocate* and *dispose*. For example, /p/ := *allocate* /Array/ [1..n] *of* /T/ allocates an array of /n/ objects of type /T/. That is, the statement allocates a contiguous chunk of memory of size /n/ times the size of an object of type /T/, and assigns a handle to this chunk (= the starting address of the chunk) to /p/. The statement *dispose* /p/ frees this memory and makes it available for reuse. With *allocate* and *dispose*, we can cut our memory array /S/ into disjoint pieces that can be referred to separately. These functions can be implemented to run in constant time. The simplest implementation is as follows. We keep track of the used portion of /S/ by storing the index of the first free cell of /S/ in a special variable, say /free/. A call of *allocate* reserves a chunk of memory starting at /free/ and increases /free/ by the size of the allocated chunk. A call of *dispose* does nothing. This implementation is time-efficient, but *not* space-efficient. Any call of *allocate* or *dispose* takes constant time. However, the total space consumption is the total space that has ever been allocated and *not* the maximum space simultaneously used, i.e., allocated but not yet freed, at any one time. It is not known whether an arbitrary sequence of *allocate* and *dispose* operations can be realized space-efficiently and with constant time per operation. However, for all algorithms presented in this book, *allocate* and *dispose* can be realized in a time- and space-efficient way.

We borrow some composite data structures from mathematics. In particular, we use *tuples*, *sequences*, and *sets*. /Pairs/, /triples/, and other /tuples/ are written in round brackets, for example (3,1), (3,1,4) and (3,1,4,1,5). Since tuples contain only a constant number of elements, operations on them can be broken into operations on their constituents in an obvious way. /Sequences/ store elements in a specified order; for example, "/s/ = \langle3,1,4,1\rangle : /Sequence/ *of* \Z" declares a sequence /s/ of integers and initializes it to contain numbers 3, 1, 4, and 1 in that order *(!)*. Sequences are a natural abstraction of many data structures, such as files, strings, lists, stacks, and queues. In Chapter 3, we shall study many ways of representing sequences. In later chapters, we shall make extensive use of sequences as a mathematical abstraction with little further reference to implementation details. The empty sequence is /always/ written as \langle\rangle.

Sets play an important role in mathematical arguments, and we shall also use them in our pseudocode. In particular, you will see declarations such as "/M/ = {3,1,4} : /Set/ *of* \N" that are analogous to declarations of arrays or sequences. Sets are usually implemented as sequences *(!)*.

*** Statements

The simplest statement is an assignment statement /x/ := /E/, where /x/ is a variable and /E/ is an expression. An assignnment is easily transformed into a constant number of RAM instructions. For example, the statement /a/ := /a/ + /bc/ is translated into "R_{1} := R_{b} \* R_{c} ; R_{a} := R_{a} + R_{1}", where R_{a}, R_{b}, and R_{c} stand for the registers storing /a/, /b/, and /c/, respectively. From programming language C, we borrow the shorthands ++ and -- for incrementing and decrementing variables. We also use parallel assignment to several variables. For example, if /a/ and /b/ are variables of the same type, "(/a/, /b/) := (/b/, /a/)" swaps the contents of /a/ and /b/ *(!)*.

The conditional statement "*if* /C/ *then* /I/ *else* /J/", where /C/ is a Boolean expression and /I/ and /J/ are statements, translates into the following instruction sequence:

eval(/C/); ~JZ~ /sElse/, R_{c}; trans(/I/); ~J~ /sEnd/; trans(/J/),

where eval(/C/) is a sequence of instructions that evaluate the expression /C/ and leave its value in register R_{c}, trans(/I/) is a sequence of instructions that implement statement /I/, trans(/J/) implements /J/, /sElse/ is the address of the first instruction in trans(/J/), and /sEnd/ is the address of the first instruction after trans(/J/). The sequence above first evaluates /C/. If /C/ evaluate to false (= 0), the program jumps to the first instruction of the translation of /J/. If /C/ evaluates to true (= 1), the program  continues with the translation of /I/ and then jumps to the instruction after the translation of /J/. The statement "*if* /C/ *then* /I/" is a shorthand for "*if /C/ *then* /I/ *else* ;", i.e., an if-then-else with an empty "else" part.

Our written representation of programs is intended for humans and uses less strict syntax than do programming languages. In particular, we usually group statements by indentation and in this way avoid the proliferation of brackets observed in programming languages such as C that are designed as a compromise between readability for humans and for computers. We use brackets /only/ if the program would be ambiguous otherwise. For the same reason, a line break can replace a semicolon for the purpose of separating statements.

The loop "*repeat* /I/ *until* /C/" translates into trans(/I/); eval(/C/); ~JZ~ /sI/, R_{c}, where /sI/ is the address of the first instruction in trans(/I/). We shall also use many other types of loops that can be viewed as shorthands for various repeat loops [see top of p. 35 for example].

Many low-level optimizations are possible when loops are translated into RAM code. These optimizations are of *no* concern to us. For us, it is only important that the execution time of a loop can be bounded by summing the execution times of each of its iterations, including *(!)* the time needed for evaluating conditions.

*** Procedures and Functions

A subroutine with the name /foo/ is declared in the form "*Procedure* foo(/D/) /I/", where /I/ is the body of the procedure and /D/ is a sequence of variable declarations specifying the parameters of /foo/. A call of /foo/ has the form foo(/P/), where /P/ is a parameter list. The parameter list has the same length as the variable declaration list. Parameter passing is either "by value" or "by reference". Our default assumption is that basic objects such as integers and Booleans are passed by value and that complex objects such as arrays are passed by reference *(!)*. These conventions are similar to the conventions used by C and guarantee that parameter passing takes constant time *(!)*. The semantics of parameter passing is defined as follows:

 - For a value parameter /x/ of type /T/, the actual parameter must be an expression /E/ of the same type. Parameter passing is equivalent to the declaration of a local variable /x/ of type /T/ initialized to /E/.
 - For a reference parameter /x/ of type /T/, the actual parameter must be a variable of the same type and the formal parameter is simply an alternative name for the actual parameter.

As with variable declarations, we sometimes omit type declarations for parameters if they are unimportant or clear from the context. Sometimes we also declare parameters implicitly using mathematical notation. For example, the declaration *Procedure* bar(\langle a_{1} , a_{2} , ... , a_{n} \rangle) introduces a procedure whose argument is a sequence of /n/ elements of unspecified type.

Most procedure calls can be compiled into machine code by simply substituting the procedure body for the procedure call and making provisions for parameter passing; this is called *inlining (!)*. Value passing is implemented by making appropriate assignments to copy the parameter values into the local variables of the procedure. Reference passing to a formal parameter /x/ : /T/ is implemented by changing the type of /x/ to *Pointer to* /T/, replacing all occurrences of /x/ in the body of the procedure by (\*x) and initializing /x/ by the assignment /x/ := *addressof* /y/, where /y/ is the actual parameter. Inlining gives the compiler many opportunities for optimization, so that inlining is the most efficient *(!)* approach for small procedures and for procedures that are called from only a single place.

*Functions* are similar to procedures, except that they allow the return statement to return a value *(!)*. Figure 2.2 on page 36 shows the declaration of a recursive function that returns ~n!~ and its translation into RAM code. The substitution approach fails for *recursive* procedures and functions that directly or indirectly call themselves - substitution would /never/ terminate *(!)*. Realizing recursive procedures in RAM code requires the concept of a *recursion stack*. Explicit subroutine calls over a stack are also used for large procedures that are called multiple times where inlining would unduly increase the code size. The recursion stack is a reserved part of the memory *(!)*. Register R_{r} always points to the first free entry in this stack. The stack contains a sequence of *activation records*, one for each active procedure call. The activation record for a procedure with /k/ parameters and /l/ local variables has size 1 + /k/ + /l/. The first location contains the return address, i.e., the address of the instruction where execution is to be continued after the call has terminated, the next /k/ locations are reserved for the parameters, and the final /l/ locations are reserved for the local variables. A procedure call is now implemented as follows:

  - First, the calling procedure *caller* pushed the return address and the actual parameters onto the stack, increases R_{r} accordingly, and jumps to the first instruction of the called routine *called*.
  - The called routine reserves space for its local variables by increasing R_{r} appropriately.
  - Then the body of *called* is executed. During execution of the body, any access to the i^{th} formal parameter (0 \le /i/ \lt /k/) is an access to S[ R_{r} - /l/ - /k/ + /i/ ] and any access to the i^{th} local variable (0 \le /i/ \lt /l/) is an access to S[ R_{r} - /l/ + /i/ ]. When *called* executed a *return* statement, it decreases R_{r} by 1 + /k/ + /l/ (observe that *called* knows /k/ and /l/) and execution continues at the return address (which can be found at S[R_{r}]). Thus, control is returned to the *caller*. Note that recursion is no problem with this scheme, since each incarnation of a routine will have its /own stack area/ for its parameters and local variables. Figure 2.3 on page 36 shows the contents of the recursion stack of a call ~factorial(5)~ when the recursion has reached ~factorial(3)~. The label ~afterCall~ is the address of the instructionfollowing the call ~factorial(5)~, and ~aRecCall~ is defined in Fig. 2.2.

*** Object Orientation

We also need a simpler form of object-oriented programming so that we can separate the interface and the implementation of the data structures *(!)*. We introduce our notation by way of example. The following definition gives a (partial) implementation of a complex number type that can use arbitrary numeric types such as \Z, \Q, and \R for the real and imaginary parts.


*Class* Complex(/x/, /y/ : /Number/) *of* /Number/
   /re/ = /x/ : /Number/
   /im/ = /y/ : /Number/
   *Function* /abs/ : /Number/ *return* \sqrt{re^{2} + im^{2}}
   *Function* add(c' : /Complex/) : /Complex/ *return* Complex(re + c'.re, im + c'.im)


Our class names (here "/Complex/") will usually begin with capital letters. The real and imaginary parts are stored in the *member variables* /re/ and /im/, respectively. Now, the declaration "/c/ : Complex(2,3) *of* \R" declares a complex number /c/ initialized to  2 + 3i, where /i/ is the imaginary unit. The expression /c.im/ evaluates to the imaginary part of /c/, and /c.abs/ returns the absolute value of /c/, a real number.

The type after the *of* allows us to parameterize classes with types in a way similar to the template mechanism of C++ or the generic types of Java. Note that in the light of this notation, the types "/Set/ *of* /Element/" and "/Sequence/ *of* /Element/" mentioned earlier are ordinary classes *(!)*. Objects of a class are initialized by setting the member variables as specified in the class definition.

** Parallel Machine Models

We classify parallel machine models into two broad classes:

  1. shared-memory machines, and
  2. distributed-memory machines.

In both cases, we have /p processing elements/ (PEs). In the former case, these PEs share a common memory and all communication between PEs is through the shared memory. In the latter case, each PE has its own private memory, the PEs are connected by a communication network, and all communication is through the network.

*** Shared-Memory Parallel Computing
