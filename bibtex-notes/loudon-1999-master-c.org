#+TITLE: Notes on: Mastering algorithms with C by Loudon, K. (1999)
#+Time-stamp: <2021-05-28 15:15:49 boxx>

- source :: cite:loudon-1999-master-c

* TODO Summary

* TODO Comments

* TODO Topics

** Pointer Fundamentals

Recall that a pointer is simply a variable that stores the address where a piece of data resides in memory rather than storing the data itself. That is, pointers contain memory addresses. One of the best things we can do to understand and communicate information about pointers is to draw diagrams. Rather than listing actual addresses in diagrams, pointers are usually drawn as arrows linking one location to another. When a pointer points to nothing at all - that is, when it is set to NULL - it is illustrated as a line terminated with a double bar.

As with other types of variables, we should not assume that a pointer points to anywhere useful until we explicitly set it. It is also important to remember that nothing  prevents a pointer in C from pointing to an invalid address (!). Pointers that point to invalid addresses are sometimes called /dangling pointers/. Some examples of programming errors that can lead to dangling pointers include casting arbitrary integers to pointers, adjusting pointers beyond the bounds of arrays, and deallocating storage that one or more pointers still reference.

** Storage Allocation

When we declare a pointer in C, a certain amount of space is allocated for it, just as for other types of variables. *Pointers generally occupy one machine word, but their size can vary*. Therefore, for portability, we should _never_ assume that a pointer has a specific size. Pointers often vary in size as a result of compiler settings and type specifiers allowed by certain C implementations. It is also important to remember that when we declare a pointer, space is allocated *only* for the pointer itself; *no space* is allocated for the data the pointer references. Storage for the data is allocated in one of two ways: by declaring a variable for it or by allocating storage dynamically at runtime (using /malloc/ or /realloc/, for example).

When we declare a variable, its type tells the compiler how much storage to set aside for it as the program runs. Storage for the variable is allocated automatically, but it may not be persistent throughout the life of the program. This is especially important to remember when dealing with pointers to /automatic variables/. Automatic variables are those for which storage is allocated and deallocated automatically when entering and leaving a block or function. For example, since ~iptr~ is set to the address of the automatic variable ~a~ in the following function ~f~, ~iptr~ becomes a dangling pointer when ~f~ returns. This situation occurs because once ~f~ returns, ~a~ is no longer valid on the program stack.

#+begin_src C
int f(int **iptr) {

int a = 10;
*iptr = &a;

return 0;

}
#+end_src

In C, when we dynamically allocate storage, we get a pointer to some storage on the heap. Since it is then our responsibility to manage this storage ourselves, the storage remains valid until we explicitly deallocate it. For example, the storage allocated by /malloc/ in the following code remains valid until we call /free/ at some later time. Thus, it remains valid even after ~g~ returns, unlike the storage allocated automatically for ~a~ previously. The parameter ~iptr~ is a pointer to the object we wish to modify (another pointer) so that when ~g~ returns, ~iptr~ contains the address returned by /malloc/. This idea is explored further in the section on pointers as parameters to functions.

#+begin_src C
#include <stdlib.h>

int g(int **iptr) {

if ((*iptr = (int *)malloc(sizeof(int))) == NULL)
return -1;

return 0;

}
#+end_src

Pointers and storage allocation are arguably the areas of C that provide the most fodder for the language's sometimes bad reputation. The misuse of dynamically allocated storage, in particular, is a notorious source of /memory leaks/. Memory leaks are blocks of storage that are allocated but never freed by a program, even when no longer in use. They are particularly detrimental when found in sections of code that are executed repeatedly. Fortunately, we can greatly reduce memory leaks by employing consistent approaches to how we manage storage.

One example of a consistent approach to storage management is the one used for data structures presented in this book. The philosophy followed in every case is that it is the responsibility of the user to manage the storage associated with the actual data that the data structure organizes; the data structure itself allocates storage /only/ for internal structures used to keep the data organized (!). Consequently, only pointers are maintained to the data inserted into the data structure, rather than private copies of the data. One important implication of this is that a data structure's implementation *does not depend on the type and size of the data it stores*. Also, multiple data structures are able to operate on a single copy of the data, which can be useful when organizing large amounts of data.

In addition, this book provides operations for /initializing/ and /destroying/ data structures. Initialization may involve many steps, one of which may be the allocation of memory. Destroying a data structure generally involves removing all of its data and freeing the memory allocated in the data structure. This is the one exception to having the user manage storage for the data. Since managing this storage is an application-specific operation, each data structure uses a function provided by the user when the data structure is initialized.

** Aggregates and Pointer Arithmetic

One of the most common uses of pointers in C is referencing /aggregate data/. Aggregate data is data composed of multiple elements grouped together because they are somehow related. C supports two (2) classes of aggregate data: /structures/ and /arrays/. (Unions, although similar to structures, are considered formally to be in a class by themselves.)

*** Structures

Structures are sequences of usually heterogeneous elements grouped so that they can be treated together as a single coherent datatype. Pointers to structures are an important part of building data structures. Whereas structures allow us to group data into convenient bundles, pointers let us link these bundles to one another in memory. By linking structures together, we can organize them in meaningful ways to help solve real problems.

As an example, consider chaining a number of elements together in memory to form a /linked list/. To do this, we might use a structure like ~ListElmt~ in the following code. Using a ~ListElmt~ structure for each element in the list, to link a sequence of list elements together, we set the ~next~ member of each element to point to the element that comes after it. We set the ~next~ member of the last element to NULL to mark the end of the list. We set the ~data~ member of each element to point to the data that the element contains. Once we have a list containing elements linked in this way, we can traverse the list by following one ~next~ pointer after another.

#+begin_src C
typedef struct ListElmt_ {

void             *data;
struct ListElmt_ *next;

} ListElmt;
#+end_src

The ~ListElmt~ structure illustrates another important aspect about pointers with structures: structures are *not* permitted to contain instances of themselves, but they may contain /pointers/ to instances of themselves (!). This is an important idea in building data structures because many data structures are built from components that are self-referential. In a linked list, for example, each ~ListElmt~ structure points to another ~ListElmt~ structure. Some data structures are even built from structures containing multiple pointers to structures of the same type. In a binary tree, for example, each node has pointers to two other binary tree nodes.

*** Arrays

Arrays are sequences of homogeneous elements arranged consecutively in memory. In C, arrays are closely related to pointers. In fact, when an array identifier occurs in an expression, C converts the array transparently into an unmodifiable pointer that points to the array's first element. Considering this, the two following functions are equivalent.


| Array Reference   | Pointer Reference |
| int f() {         | int g() {         |
| int a[10], *iptr; |                   |
| iptr = a;         | int a[10], *iptr; |
| iptr[0] = 5;      | iptr = a;         |
|                   | *iptr = 5;        |
| return 0;         | return 0;         |
|                   | }                 |
| }                 |                   |

To understand the relationship between arrays and pointers in C, recall that to access the i^th element in an array ~a~, we use the expression:

~a[i]~

The reason that this expression accesses the i^th element of ~a~ is that C treats ~a~ in this expression the same as a pointer that points to the first element of ~a~. The expression as a whole is equivalent to the following:

~*(a + i)~

which is evaluated using the rules of /pointer arithmetic/. Simply stated, when we add an integer /i/ to a pointer, the result is the address, plus /i/ times the number of bytes in the datatype the pointer references; it is not simply the address stored in the pointer plus /i/ bytes. An analogous operation is performed when subtract an integer from a pointer. This explains why arrays are zero-indexed in C; that is, the first element in an array is at position 0.

For example, if an array or pointer contains the address 0x10000000, at which a sequence of five 4-byte integers are stored, ~a[3]~ accesses the integer at address 0x1000000c. This address is obtained by adding (3)(4) = 12_10 = c_16 to the address 0x10000000. On the other hand, for an array or pointer reference twenty characters (a string), ~a[3]~ accesses the character at address 0x10000003. This address is obtained by adding (3)(1) = 3_10 = 3_16 to the address 0x10000000. Of course, an array or pointer referencing one piece of data looks no different from an array or pointer referencing many pieces. Therefore, it is important to keep track of the amount of storage that a pointer or array references and to not access addresses beyond this.

The conversion of a multidimensional array to a pointer is analogous to converting a one-dimensional array. *However, we also must remember that in C, multidimensional arrays are stored in row-major order*. This means that subscripts to the right vary more rapidly than those to the left. To access the element at row /i/ and column /j/ in a two-dimensional array, we use the expression:

~a[i][j]~

C treats ~a~ in this expression as a pointer that points to the element at row 0, column 0 in ~a~. The expression as a whole is equivalent to:

~*(*(a + i) + j)~

*** Pointers as Parameters to Functions

Pointers are an essential part functions in C. Most importantly, they are used to support a type of parameter passing called by /call-by-reference/. In call-by-reference parameter passing, when a function changes a parameter passed to it, the change persists after the function returns. Contrast this with /call-by-value/ parameter passing, in which changes to parameters persist /only/ within the function itself. Pointers are also an efficient means of passing large amounts of data in and out of functions, whether we plan to modify the data or not. This method is efficient because only a pointer is passed instead of a complete copy of the data. This technique is used in many of the examples in this book.

*** Call-by-Reference Parameter Passing

Formally, C supports /only/ call-by-value parameter passing. In call-by-value parameter passing, private copies of a function's calling parameters are made for the function to use as it executes. *However, we can simulate call-by-reference parameter passing by passing pointers to parameters instead of passing the parameters themselves*. Using this approach, a function gets a private copy of a pointer to each parameter in the caller's environment.

To understand how this works, first consider ~swap1~, which illustrates an _incorrect_ implementation of a function to swap two integers using call-by-value parameter passing without pointers. The function ~swap2~ corrects the problem by using pointers to simulate call-by-reference parameter passing.

_Incorrect Swap_

#+begin_src C
void swap1(int x, int y) {

int tmp;
tmp = x; x = y; y = tmp;

return;

}
#+end_src

_Correct Swap_

#+begin_src  C
void swap2(int *x, int *y) {

int tmp;
tmp = *x; *x = *y; *y = tmp;

return;

}
#+end_src

One of the nice things about C and call-by-reference parameter passing is that the language gives us complete control over exactly (!) how parameter passing is performed. One disadvantage, however, is that this control can be cumbersome since we often end up having to dereference call-by-reference parameters numerous times in functions.

Another use of pointers in function calls occurs when we pass arrays to functions. Recalling that C treats all array names transparently as unmodifiable pointers, passing an array of objects of type ~T~ in a function is equivalent to passing a pointer to an object of type ~T~. *Thus, we can use the two (2) approaches interchangeably*. For example, function ~f1~ and function ~f2~ are equivalent.

_Array Reference_

#+begin_src  C
int f1(int a[]) {

a[0] = 5;

return 0;

}
#+end_src

_Pointer Reference_

#+begin_src C
int f2(int *a) {

*a = 5;

return 0;

}
#+end_src

Usually the approach chosen depends on a convention or on wanting to convey something about how the parameter is used in the function. When using an array parameter, bounds information is often omitted since it is *not* required by the compiler. However *(!)*, including bounds information can be a useful way to document a limit the function imposes on a parameter internally. Bounds information plays a more critical with array parameters that are multidimensional.

When defining a function that accepts a multidimensional array, /all but the first dimension must be specified/ so that pointer arithmetic can be performed when elements are accessed, as shown in the following code:

#+begin_src C
int g(int a[][2]) {

a[2][0] = 5;

return 0;

}
#+end_src

To understand why we must include all but the first dimension, imagine a two-dimensional array of integers with three rows and two columns. *In C, elements are stored in row-major order at increasing addresses in memory*. This means that the two integers in the first row are stored first, followed by the two integers in the second row, followed by the two integers in the third row. *Therefore, to access an element in any row but the first, we must know exactly how many elements to skip in each row to get to elements in successive rows*.

*** Pointers to Pointers as Parameters

One situation in which pointers are used as parameters to functions a great deal in this book is when /a function must modify a pointer passed into it/. To do this, the function is passed a *pointer to the pointer* to be modified. Consider the operation ~list_rem_next~, which Chapter 5 defines for removing an element from the linked list. Upon return, ~data~ points to the data removed from the list:

#+begin_src C
int list_rem_next(List *list, listElmt *element, void **data);
#+end_src

Since the operation must modify the pointer ~data~ to make it point to the data removed, we must pass the address to the pointer ~data~ in order to simulate call-by-reference parameter passing. *Thus, the operation takes a pointer to a pointer as its third parameter*. This is typical of how data is removed from most of the data structures presented in this book.

** Generic Pointers and Casts

Recall that pointer variables in C have types just like other variables (!). The main reason for this is so that when we dereference a pointer, the compiler knows the type of data being pointed to and can access the data accordingly. However, sometimes we are not concerned about the type of data a pointer references. In these cases we use /generic pointers/, which bypass C's type system.

*** Generic Pointers

Normally C allows assignments only between pointers of the same type. For example, given a character pointer ~sptr~ (a string) and an integer pointer ~iptr~, we are *not* permitted to assign ~sptr~ to ~iptr~ or vice versa. However, generic pointers can be set to pointers of _any_ type. Thus, given a generic pointer ~gptr~, we are permitted to assign ~sptr~ to ~gptr~ or ~gptr~ to ~sptr~. *To make a pointer generic in C, we declare it as a void pointer*.

There are many situations in which void pointers are useful. For example, consider the standard C library function ~memcpy~, which copies a block of data from one location in memory to another. Because ~memcpy~ may be used to copy data of any type, it makes sense that its pointer parameters are void pointers. Void pointers can be used to make other types of functions more general as well. For example, we might have implemented the ~swap2~ function presented earlier so that it swapped data of any type, as shown in the following code:

#+begin_src C
#include <stdlib.h>
#include <string.h>

int swap2(void *x, void *y, int size) {

void *tmp;

if ((tmp = malloc(size)) == NULL)
return -1;

memcpy(tmp, x, size); memcpy(x, y, size); memcpy(y, tmp, size);
free(tmp);

return 0;

}
#+end_src

Void pointers are particularly useful when implementing data structures because they allow us to store and retrieve data of any type. Consider again the ~ListElmt~ structure presented earlier for linked lists. Recall that this structure contains two (2) members, ~data~ and ~next~. Since ~data~ is declared as a void pointer, it can point to data of any type. Thus, we can use ~ListElmt~ structures to build _any_ type of list.

In Chapter 5, one of the operations defined for linked lists is ~list_ins_next~, which accepts a void pointer to the data to be inserted:

#+begin_src C
int list_ins_next(List *list, ListElmt *element, void *data);
#+end_src

To insert an integer referenced by ~iptr~ into a list of integers, ~list~, after an element referenced by ~element~, we use the following call. C permits us to pass the integer pointer ~iptr~ for the parameter ~data~ because ~data~ is a void pointer.

#+begin_src C
retval = list_ins_next(&list, element, iptr);
#+end_src

Of course, when removing data from the list, it is important to use the correct type of pointer to retrieve the data removed. Doing so ensures that the data will be interpreted correctly if we try to do something with it. As discussed earlier, the operation for removing an element from a linked list is ~list_rem_next~, which takes a pointer to a void pointer as its third parameter:

#+begin_src C
int list_rem_next(List *list, ListElmt *element, void **data);
#+end_src

To remove an integer from ~list~ after an element referenced by ~element~, we use the following call. Upon return, ~iptr~ points to the data removed. We pass the address of the pointer ~iptr~ since the operation modifies the pointer itself to make it point to the data removed.

#+begin_src C
retval = list_rem_next(&list, element, (void **)&iptr);
#+end_src

This call also includes a ~cast~ to make ~iptr~ temorarily appear as a pointer to a void pointer, since this is what ~list_rem_next~ requires. As we will see in the next section, *casting is a mechanism in C that lets us temporarily treat a variable of one type as a variable of another type*. A cast is necessary here because, although a void pointer is compatible with any other type of pointer in C, a pointer to a void pointer is not *(!)*.

*** Casts

To cast a variable ~t~ of some type ~T~ to another type ~S~, we precede ~t~ with ~S~ in parentheses. For example, to assign an integer pointer ~iptr~ to a floating-point pointer ~fptr~, we cast ~iptr~ to a floating-point pointer and then carry out the assignment, as shown:

#+begin_src C
fptr = (float *)iptr;
#+end_src

(Although casting an integer pointer to a floating-point pointer is a _dangerous_ practice in general, it is presented here as an illustration.)

After the assignment, ~iptr~ and ~fptr~ both contain the same address. However, the interpretation of the data at this address depends on which pointer we use to access it *(!)*.

Casts are especially important with generic pointers because /generic pointers cannot be dereferenced without casting them to some other type/. This is because generic pointers give the compiler *no* information about what is being pointed to; thus, it is not clear how many bytes should be accessed, nor how the bytes should be interpreted. Casts are also a nice form of self-documentaion when generic pointers are assigned to pointers of other types. Although the cast is not necessary in this case, it does improve a program's readability.

When casting pointers, one issue we need to be particularly sensitive to is the way data is aligned in memory. Specifically, we need to be aware that applying casts to pointers can undermine the alignment a computer expects. Often computers have alignment requirements so that certain hardware optimizations can make accessing memory more efficient. For example, a system may insist that all integers be aligned on word boundaries. Thus, given a void pointer that is not word aligned, if we cast the void pointer to an integer pointer and dereference it, we can expect an exception to occur at runtime.

** Function Pointers

Function pointers are pointers that, instead of pointing to data, point to executable code or blocks of information needed to invoke executable code. They are used to store and manage functions as if they were pieces of data. Function pointers have a type that is described in terms of a return value and parameters that the function accepts. Declarations for function pointers look much like declarations for functions, except *(!)* that an asterisk appears before the function name, and the asterisk and name are surrounded by parentheses *for reasons of associativity*. For example, in the following code, ~match~ is declared as a pointer to a function that accepts two (2) void pointers and returns an integer:

#+begin_src C
int (*match)(void *key1, void *key2);
#+end_src

This declaration means that we can set ~match~ to point to any function that accepts two (2) void pointers and returns an integer. For example, suppose ~match_int~ is a function that accepts two (2) void pointers to integers and returns ~1~ if the integers match, or ~0~ otherwise. Assuming the previous declaration, we could set ~match~ to point to this function by executing the following statement:

#+begin_src C
match = match_int;
#+end_src

To execute a function referenced by a function pointer, we simply use the function pointer wherever we would normally use the function itself. For example, to invoke the function referenced by ~match~ earlier, we execute the following statement, assuming ~x~, ~y~, and ~retval~ have been declared as integers:

#+begin_src C
retval = match(&x, &y);
#+end_src

One important use of function pointers in this book is to encapsulate functions into data structures. For example, in the implementation of chained hash tables, the data structure has a ~match~ member similar to the function pointer just described. This pointer is used to invoke a function whenever we need to determine whether an element we are searching for matches an element in the table. We assign a function to this pointer when the table is initialized. The function we assign has the same prototype as ~match~ but internally compares two (2) elements of the appropriate type, depending on the type of data in the table for which the table has been defined. *Using a pointer to store a function as part of a data structure is nice because it is yet another way to keep an implementation generic*.

** Recursion

Recursion is a powerful principle that allows something to be defined in terms of smaller instances of itself. Perhaps there is no better way to appreciate the significance of recursion than to look at the mysterious ways nature uses it. Think of the fragile leaf of a fern, in which each individual sprig from the leaf's stem is just a smaller copy of the overall leaf. Think of the repeating patterns in a reflection, in which two shiny objects reflect each other. Examples like these convince us that even though nature is a great force, in many ways it has a paradoxical simplicity that is truly elegant. The same can be said for recursive algorithms; in many ways, recursive algorithms are simple and elegant, yet they can be extremely powerful.

In computing, recursion is supported via recursive functions. A recursive function is a function that calls itself. Each successive call works on a more refined set of inputs, bringing us closer and closer to the solution of a problem. Most developers are comfortable with the idea of dividing a larger problem into several smaller ones an writing separate functions to solve them. However, many developers are less comfortable with the idea of solving a larger problem with a single function that calls itself. Admittedlt, looking at a problem in this way can take some getting used to. This chapter explores how recursion works and shows how to define some problems in a recursive manner. Some examples of recursive approaches in this book are found in tree traversals, breadth-first and depth-first searches with graphs, and sorting.

** Basic Recursion

To begin, let's consider a simple problem that normally we might not think of in a recursive way. Suppose we would like to compute the factorial of a number ~n~. The factorial of ~n~, written as ~n!~, is the product of all numbers from ~n~ down to ~1~. One way to calculate this is to loop through each number and multiply it with the product of all preceding numbers. This is an /iterative/ approach.

Another way to look at this problem is to define ~n!~ as the product of smaller factorials. To do this, we define ~n!~ as ~n~ times the factorial ~n-1~. Of course, solving ~(n-1)!~ is the same problem as ~n!~, only a little smaller. If we then think of ~(n-1)!~ as ~n-1~ times ~(n-2)!~, ... and so forth until ~n=1~, we end up computing ~n!~. This is a /recursive/ approach, which can be defined more formally as:

#+begin_src latex
\begin{align*}
F(n) =
\left\{
1 & if n=0, n = 1 \\
nF(n-1) & if n > 1
\right.
\end{align*}
#+end_src

The two (2) basic phases of a recursive process are (1) /winding/ and (2) /unwinding/. In the winding phase, each recursive call perpetuates the recursion by making an additional recursive call itself. The winding phase terminates when one of the calls reaches a /terminating condition/. A terminating condition defines the state at which a recursive function should return instead of making another recursive call. For example, in computing the factorial of ~n~, the terminating conditions are ~n=1~ and ~n=0~, for which the function simply returns ~1~. Every recursive function *must* have at least one terminating condition; otherwise the winding phase never terminates. Once the winding phase is complete, the process enters the unwinding phase, in which previous instances of the function are revisited *in reverse order*. This phase continues until the original call returns, at which point the recursive process is complete.

To understand how recursion really works, it helps to look at the way functions are executed in C. For this, we need to understand a little about the organization of a C program in memory. Fundamentally, a C program consists of four (4) areas as it executes:

 1) *code area* - contains the machine instructions that are executed as the program runs
 2) *static data area* - contains data that persists throughout the life of the program, such as global variables and static local variables
 3) *heap* - contains dynamically allocated storage, such as memory allocated by ~malloc~
 4) *stack* - contains information about function calls

By convention, the heap grows upward from one end of a program's memory, while the stack grows downward from the other end (but this may vary in practice). Note that the term /heap/ as it used in this context has *nothing* to do with the heap data structure *(!)*

When a function is called in a C program, a block of storage is allocated on the stack to keep track of information associated with the call. Each call is referred to as an /activation/. The block of storage placed on the stack is called an /activation record/ or, alternatively, a /stack frame/. An activation record consists of five (5) regions:

 1) *incoming parameters* - the parameters passed into the activation
 2) *space for a return value* - the parameters passed to functions called within the activation
 3) *temporary storage used in evaluating expressions*
 4) *saved state information for when the activation terminates*
 5) *outgoing parameters*

The outgoing parameters of one activation record become the incoming parameters of the next one placed on the stack. The activation record for a function call remains on the stack until the call terminates.

The stack is a great solution to storing information about function calls because its last-in, first-out behavior is well suited to the order in which functions are called and terminated. However *(!)*, stack usage does have a few drawbacks. Maintaining information about every function call until it returns takes a considerable amount of space, especially in programs with many recursive calls. In addition, generating and destroying activation records takes time because there is a significant amount of information that must be saved and restored. Thus, if the overhead associated with these concerns becomes too great, we may need to consider an iterative approach. Fortunately, we can use a special type of recursion, called /tail recursion/, to avoid these concerns in some cases.

** Tail Recursion

A recursive function is said to be /tail recursive/ if all recursive calls within it are tail recursive. A recursive call is tail recursive when it is the last statement that will be executed within the body of a function and its return value is *not* part of an expression. Tail-recursive functions are characterized as having *nothing to do during the unwinding phase*. This characteristic is important because most modern compilers automatically generate code to take advantage of it.

When a compiler detects a call that is tail recursive, it overwrites the current activation record instead of pushing a new one onto the stack. The compiler can do this because the recursive call is the last statement to be executed in the current activation; thus, there is nothing left to do in the activation when the call returns. Consequently, there is *no* reason to keep the current activation record instead of stacking another one on top of it, stack usage is greatly reduced, which leads to better performance in practice. Thus, we should make recursive functions tail recursive whenever we can.

** Analysis of Algorithms

Whether we are designing an algorithm or applying one that is widely accepted, it is important to understand how the algorithm will perform. There are a number of ways we can look at an algorithm's performance, but usually the aspect of most interest is how fast the algorithm will run. In some cases, if an algorithm uses significant storage, we may alos be interested in its space requirement as well. Whatever the case, determining how an algorithm performs requires a formal and deterministic method.

There are many reasons to understand the performance of an algorithm. For example, we often have a choice of several algorithms when solving problems. Understanding how each performs helps us to differentiate between them. Understanding the burden an algorithm places on an application also helps us plan how to use the algorithm more effectively. For instance, /garbage collection algorithms,/ algorithms that collect dynamically allocated storage to return to the heap require considerable time to run. Knowing this, we can be careful to run them only at opportune moments, just as LISP and Java do, for example.

** Worst-Case Analysis

Most algorithms do not perform the same in all cases; normally an algorithm's performance varies with the data passed to it. Typically, three (3) cases are recognized: /best case/, /worst case/, and /average case/. For any algorithm, understanding what constitutes each of these cases is an important part of analysis because performance can vary significantly between them. Consider even a simple algorithm such as /linear search/. Linear search is a natural but inefficient search technique in  which we look for an element simply by traversing a set from one end to the other. In the best case, the element we are looking for is the first element we insoect, so we end up traversing only a single element. In the worst case, however, the desired element is the last one we inspect, in which case we end up traversing all of the elements. On average, we can expect to find the element somewhere in the middle.

*** Reasons for Worst-Case Analysis

A basic understanding of how an algorithm performs in all cases is important, but usually we are most interested in how an algorithm performs in the /worst case/. There are four (4) reasons why algorithms are generally analyzed by their worst case:

  - Many algorithms perform to their worst case a large part of the time. For example, the worst case in searching occurs when we do not find what we are looking for at all. Imagine how frequently this takes place in some database applications.

  - The best case is not very informative because many algorithms perform exactly the same in the best case. For example, nearly all searching algorithms can locate an element in one inspection at best, so analyzing this case does not tell us much.

  - Determining average-case performance is not always easy. Often it is difficult to determine exactly what the "average case" even is. Since we can seldom guarantee precisely how an algorithm will be exercised, usually we cannot obtain an average-case measurement that is likely to be accurate.

  - The worst case gives us an upper bound on performance. Analyzing an algorithm's worst case guarantees that it will /never/ perform worse than what we determine. Therefore, we know that the other cases /must/ perform at least as well.

Although worst-case analysis is the metric for many algorithms, it is worth noting that there are exceptions. Sometimes special circumstances let us base performance on the average case. For example, randomized algorithms such as quicksort use principles of probability to virtually guarantee average-case performance.

*** /O/-Notation

/O/-notation is the most common notation used to express an algorithm's performance in a formal manner. *Formally, /O/-notation expresses the upper bound of a function within a constant factor*. Specifically, if /g(n)/ is an upper bound of /f(n)/, then for some constant /c/ it is possible to find a value of /n/, call it n_0, for which any value of n \ge n_0 will result in /f(n) \le cg(n)/.

Normally we express an algorithm's performance as a function of the size of the data it processes. That is, for some data of size /n/, we describe its performance with some function /f(n)/. However, while in many cases we can determine /f/ exactly, usually it is not necessary to be this precise. *Primarily we are interested only in the /growth rate/ of /f/, which describes how quickly the algorithm's performance will degrade as the size of the data it processes becomes arbitrarily large*. An algorithm's growth rate, or /order of growth/, is significant because ultimately it describes how efficient the algorithm is for arbitrary inputs. /O/-notation reflects an algorithm's order of growth.

*** Simple Rules for /O/-Notation

When we look at some function /f(n)/ in terms of its growth rate, a few things become apparent. First, we can ignore constant terms because as the value of /n/ becomes larger and larger, eventually constant terms will become insignificant. Second, we can ignore constant multipliers of terms because they too will become insignificant as the value of /n/ increases. Finally *(!)*, we need only consider the highest-order term because, again, as /n/ increases, higher-order terms quickly outweigh the lower-order terms.

These ideas are formalized in the following simple rules for expressing functions in /O/-notation.

  - Constant terms are expressed as O(1). When analyzing the running time of an algorithm, apply this rule when you have a task that you know will execute in a certain amount of time regardless of the size of the data it processes. Formally states, for some constant /c/, /O(c) = O(1)/.

  - Multiplicative constants are omitted. When analyzing the running time of an algorithm, apply this rule when you have a number of tasks that all execute in the same amount of time. For example, if three (3) tasks each run in time /T(n) = n/, the result is /O(3n)/, which simplifies to /O(n)/. Formally stated, for some constant /c/, /O(cT) = cO(T) = O(T)/.

  - Addition is performed by taking the maximum. When analyzing the running time of an algorithm, apply this rule when one task is executed after another. For example, if T_1(n) = /n/ and T_2(n) = n^2 describe the two tasks executed sequentially, the result is O(n) + O(n^2), which simplifies to O(n^2). Formally stated, O(T_1) + O(T_2) = O(T_1 + T_2) = max(O(T_1), O(T_2)).

  - Multiplication is not changed but often is rewritten more compactly. When analyzing the running time of an algorithm, apply this rule when one task causes another to be executed some number of times for each iteration of itself. For example, in a nested loop whose outer iterations are described by T_1 and whose inner iterations by T_2, if T_1(n) = /n/ and T_2(n) = /n/, the result is O(n)O(n), or O(n^2). Formally stated, O(T_1)O(T_2) = O(T_1 T_2).

** Computational Complexity

When speaking of the performance of an algorithm, usually the aspect of interest is its /complexity/, which is the growth rate of the resources (usually time) it requires with respect to the size of the data it processes. /O/-notation describes an algorithm's complexity. Using /O/-notation, we can frequently describe the worst-case complexity of an algorithm simply by inspecting its overall structure. Other times, it is more helpful to employ techniques involving recurrences and summation formulas, and statistics.

To understand complexity, let's look at one way to surmise the resources an algorithm will require. It should seem reasonable that if we look at an algorithm as a series of /k/ statements, each with some cost (usually time) to execute, c_i, we can determine the algorithm's total cost by summing the costs of all statements from c_1 to c_k in whatever order each is executed. Normally statements are executed in a more complicated manner than simply in sequence, so this has to be taken into account when totaling the costs.

When inspecting the overall structure of an algorithm, only two (2) steps need to be performed: (1) we must determine which parts of the algorithm depend on data whose size is not constant, and (2) then derive functions that describe the performance of each part. All other parts of the algorithm execute with a constant cost and can be ignored in figuring its overall complexity.

Assume that an algorithm's running time can be represented as T(n). It is important to realize that /O(n)/, its complexity, *says little about the /actual/ time the algorithm will take to run*. In other words, just because an algorithm has a low growth rate does not necessarily mean it will execute in a small amount of time. In fact, *complexities have no real units of measurement at all*. They describe only how the resource being measured will be affected by a /change/ in data size *(!)*. For example, saying that /T(n)/ is /O(n)/ conveys only that the algorithm's running time varies proportionally to /n/, and that /n/ is an upper bound for /T(n)/ within a constant factor. Formally, we say that /T(n) \le cn/, where /c/ is a constant factor that accounts for various costs *not* associated with the data, such as the type of computer on which the algorithm is running, the compiler used to generate the machine code, and constants in the algorithm itself.

Just as the complexity of an algorithm says little about its actual running time, *it is important to understand that no measure of complexity is necessarily efficient or inefficient*. Although complexity is an indication of the efficiency of an algorithm, whether a particular complexity is considered efficient or inefficent depends on the problem. Generally, an efficient algorithm is one in which we know we are doing the best we can do given certain criteria. Typically, an algorithm is said to be efficient if there are no algorithms with lower complexities to solve the same problem and the algorithm does not contain excessive constants. Some problems are intractable. so there are no "efficient" solutions without settling for an approximation. This is true of a special class of problems called /NP-complete problems/.

Although an algorithm's complexity is an important starting point for determining how well it will perform, often there are other things to consider in practice. For example, when two algorithms are of the same complexity, it may be worthwhile to consider their less significant terms and factors. If the data on which the algorithms' performances depend is small enough, even an algorithm of greater complexity with small constants may perform better in practice than one that has a lower order of complexity and larger constants. Other factors worth considering are how complicated an algorithm will be to develop and maintain, and how we can make the actual implementation of an algorithm more efficient. An efficient implementation does not always affect an algorithm's complexity, but it can reduce constant factors, which makes the algorithm run faster in practice.

** Linked Lists
