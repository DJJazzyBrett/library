#+TITLE: Notes on: Introduction to algorithms by T. H. Cormen (2009)
#+Time-stamp: <2021-06-02 20:02:17 boxx>

- source :: cite:cormen-2009-introd

* TODO Summary

* TODO Comments

* TODO Topics

** Section 2.2: Analyzing algorithms

/Analyzing/ an algorithm has come to mean predicting the resources that the algorithm requires. Occasionally, resources such as memory, communication bandwidth, or computer hardware are of primary concern, but most often it is computational time that we want to measure.

Before we can analyze an algorithm, we must have a model of the implementation technology that we will use, including a model for the resources of that technology and their costs. For most of this book, we shall assume a generic one-processor, *random-access machine (RAM)* model of computation as our implementation technology and understand that our algorithms will be implemented as computer programs. In the RAM model, instructions are executed one after another, with *no* concurrent operations.

Strictly speaking, we should precisely define the instructions of the RAM model and their costs. To do so, however, would be tedious and would yield little insight into algorithm design and analysis. Yet we must be careful *not* to abuse the RAM model. Our guide is how real computers are designed. The RAM model contains instructions commonly found in real computers:

  - arithmetic (such as add, subtract, multiply, divide, remainder, floor, ceiling)
  - data movement (load, store, copy)
  - control (conditional and unconditional branch, subroutine call and return).

Each such instruction takes a constant amount of time.

The data types in the RAM model are integer and flowting point (for storing real numbers). Although we typically do not concern ourselves with precision in this book, in some applications precision is crucial. We also assume a limit on the size of each word of data *(!)*. For example, when working with inputs of size /n/, we typically assume that integers are represented by /c/ lg /n/ bits for some constant /c/ \ge 1 so that each word can hold the value of /n/, enabling us to index the individual input elements, and we restrict /c/ to be a constant so that the word size does *not* grow arbitrarily. (If the word size could grow arbitrarily, we could store huge amounts of data in one word and operate on it all in constant time - clearly an unrealistic scenario.)

Real computers contain instructions not listed above, and such instructions represent a gray area in the RAM model. For example, is exponentiation a constant-time instruction? In the general case, *no*; it takes several instructions to compute x^y when /x/ and /y/ are real numbers. In restricted stiuations, however, exponentiation is a constant-time operation. Many computers have a "shift left" instruction, which in constant time shifts the bits of an integer by /k/ positions to the left. In most computers, shifting the bits of an integer by one position to the left is equivalent to multiplication by 2, so that shifting the bits by /k/ positions to the left is equivalent to multiplication by 2^k. Therefore, such computers can compute 2^k in one constant-time instruction by shifting the integer 1 by /k/ positions to the left, as long as /k/ is no more than the number of bits in a computer word. We will endeavor to avoid such gray areas in the RAM model, but we will treat computation of 2^k as a constant-time operation when /k/ is a small enough positive integer.

In the RAM model, we do *not* attempt to model the memory hierarchy that is common in contemporary computers. That is, we do not model caches or virtual memory. Several computational models attempt to account for memory-hierarchy effects, which are sometimes significant in real programs on real machines. A handful of problems in this book examine memory-hierarchy effects, but for the most part, the analyses in this book will *not* consider them. Models that include the memory hierarchy are quite a bit more complex than the RAM model, and so they can be difficul to work with. Moreover, RAM-model analyses are usually excellent predictors of performance on actual machines.

Analyzing even a simple algorithm in the RAM model can be a challenge. The mathematical tools required may include combinatorics, probability theory, algebraic dexterity, and the ability to find the most significant terms in a formula. Because the behavior of an algorithm may be different for each possible input, we need a means for summarizing that behavior in simple, easily understood formulas.

Even though we typically select only one machine model to analyze a given algorithm, we still face many choices in deciding how to express our analysis. We would like a way that is simple to write and manipulate, shows the important characteristics of an algorithm's resource requirements, and suppresses tedious details.

*** Analysis of insertion sort

The time taken by the ~INSERTION-SORT~ procedure depends on the input; sorting a thousand numbers takes longer than sorting three numbers. Moreover, ~INSERTION-SORT~ can take different amounts of time to sort two input sequences of the same size depending on how nearly sorted they are. In general, the time taken by an algorithm grows with the size of the input, so it is traditional to describe the running time of a program as a function of the size of its input. To do so, we need to define the terms "running time" and "size of input" more carefully.

The best notion for *input size* depends on the problem being studied. For many problems, such as sorting or computing discrete /Fourier Transforms/, the most natural measure is the /number of items in the input/ - for example, the array size /n/ for sorting. For many other problems, such as multiplying two integers, the best measure of input size is the /total number of bits/ needed to represent the input in ordinary binary notation. Sometimes, it is more appropriate to describe the size of the input with two numbers rather than one. For instance, if the input to an algorithm is a graph, the input size can be described by the numbers of vertices and edges in the graph. We shall indicate which input size measure is being used with each problem we study.

The *running time* of an algorithm on a particular input is the number of primitive operations or "steps" executed. It is convenient to define the notion of step so that it is as machine-independent as possible. For the moment, let us adopt the following view. A constant amount of time is required to execute each line of our pseudocode. One line may take a different amount of time than another line, but we shall assume that each execution of the  i^th line takes time c_i, where c_i is a constant. This viewpoint is in keeping with the RAM model, and it also reflects how the pseudocode would be implemented on most actual computers.

We define /T(n)/ to be the running time of a particular algorithm on an input of /n/ values. Typically, as in insertion sort, the running time of an algorithm is fixed for a given input, although in later chapters we shall see some interesting "randomized" algorithms whose behavior can vary even for a fixed input.

*** Worst-case and average-case analysis

In our analysis of insertion sort, we looked at both the best case, in which the input array was already sorted, and the worst case, in which the input array was reverse sorted. For the remainder of this book, though, we shall usually concentrate on finding only the *worst-case running time*, that is, the longest running time for /any/ input of size /n/. We give three (3) reasons for this orientation.

  1. The worst-case running time of an algorithm gives us an upper bound on the running time for any input. Knowing it provides a guarantee that the algorithm will never take any longer. We need not make some educated guess about the running time and hope that it never gets much worse.

  2. For some algorithms, the worst case occurs fairly often. For example, in searching a database for a particular piece of information, the searching algorithm's worst case will often occur when the information is not present in the database. In some applications, searches for absent information may be frequent.

  3. The "average case" is often roughly as bad as the worst case. Suppose that we randomly choose /n/ numbers and apply insertion sort. How long does it take to determine where in subarray A[1..j-1] to insert element A[j]? On average, half the elements in A[1..j-1] are less than A[j], and half the elements are greater. On average, therefore, we check half of the subarray A[1..j-1], and so t_j is about j/2. The resulting average-case running time turns out to be a quadratic function of the input size, just like the worst-case running time.

In some particular cases, we shall be interested in the *average-case* running time of an algorithm; we shall see the technique of *probabilistic analysis* applied to various algorithms throughout this book. The scope of average-case analysis is limited, because it may not be apparent what constitutes an "average" input for a particular problem. Often, we shall assume that all inputs of a given size are equally likely. In practice, this assumption may be violated, but we can sometimes use a *randomized algorithm*, which makes random choices, to allow a probabilistic analysis and yield an *expected* running time.

*** Order of growth

We used some simplifying abstractions to ease our analysis of the ~INSERTION-SORT~ procedure. First, we ignored the actual cost of each statement, using the constants c_i to represent these costs. Then, we observed that even these constants give us more detail than we really need: we expressed the worst-case running time as an^2 + bn + c for some constants a, b, and c that depend on the statement costs c_i. We thus ignored not only the actual statement costs, but also the abstract costs c_i.

We shall now make one more simplifying abstraction: it is the *rate of growth*, or *order of growth*, of the running time that really interests us. We therefore consider only the leading terms of a formula (e.g., an^2), since the lower-order terms are relatively insignificant for large values of /n/. We also ignore the leading term's constant coefficient, since constant factors are less significant that the rate of growth in determining computational efficiency for large inputs.

We usually consider one algorithm to be more efficient that another if its worst-case running time has a lower order of growth *(!)*. Due to constant factors and lower-order terms, an algorithm whose running time has a higher order of growth might take less time for small inputs than an algorithms whose running time has a lower order of growth. But for larger inputs, a \Theta(n^2) algorithm, for example, will run more quickly in the worst case than a \Theta(n^3) algorithm.


** Section 2.3.1: Designing algorthms

We can choose from a wide range of algorithm design techniques. For insertion sort, we used a *incremental approach*: having sorted the subarray A[1..j-1], we inserted the single element A[j] into its proper place, yielding the sorted subarray A[1..j].

In this section, we examine an alternative design approach, known as "divide-and-conquer", which we shall explore in more detail in Chapter 4. We'll use divide-and-conquer to design a sorting algorithm whose worst-case running time is much less than that of insertion sort. One advantage of divide-and-conquer algorithms is that their running times are often easily determine using techniques that we will see in Chapter 4.

*** The divide-and-conquer approach

Many useful algorithms are *recursive* in structure: to solve a given problem, they call themselves recursively one or more times to deal with closely related subproblems. These algorithms typically follow a *divide-and-conquer* approach: they break the problem into several subproblems that are similar to the original problem but smaller in size, solve the subproblems recursively, and then combine these solutions to create a solution to the original problem.

The divide-and-conquer paradigm involves three (3) steps at each level of recursion:

  1. *Divide* the problem into a number of subproblems that are smaller instances of the same problem.

  2. *Conquer* the subproblems by solving them recursively. If the subproblem sizes are small enough, however, just solve the subproblems in a straightforward manner.

  3. *Combine* the solutions to the subproblems into the solution for the original problem.

The *merge sort* algorithm closely follows the divide-and-conquer paradigm. Intuitively, it operates as follows.

  1. *Divide*: Divide the /n/-element sequence to be sorted into two (2) subsequences of n/2 elements each.

  2. *Conquer*: Sort the two (2) subsequences recursively using merge sort.

  3. *Combine*: Merge the two (2) sorted subsequences to produce the sorted answer.

The recursion "bottoms out" when the sequence to be sorted has length 1, in which case there is no work to be done, since every sequence of length 1 is already in sorted order.

The key operation of the merge sort algorithm is the merging of two sorted sequences in the "combine" step. We merge by calling an auxiliary procedure ~MERGE(A,p,q,r)~, where /A/ is an array and /p, q/, and /r/ are indices into the array such that /p/ \le /q/ \lt /r/. The procedue assumes that the subarrays A[p..q] and A[q+1..r] are in sorted order. It *merges* them to form a single sorted subarray that replaces the current subarray A[p..r].

Our ~MERGE~ procedure takes time \Theat(n), where n = r - p + 1 is the total number of elements being merged, and it works as follows. Returning to our card-playing motif, suppose that we have two piles of cards face up on a table. Each pile is sorted, with the smallest cards on top. We wish to merge the two piles into a single sorted output pile, which is to be face down on the table. Our basic step consists of chooseing the smaller of the two cards on top of the face-up piles, removing it from its pile (which exposes a new top card), and placing this card face down onto the output pile. We repeat this step until one input pile is empty, at which time we just take the remaining input pile and place it face down onto the output pile. Computationally, each basic step takes constant time, since we are comparing just the two top cards. Since we perform at most /n/ basic steps, merging takes \Theta(n) time.

The following pseudocode implements the above idea, but *(!)* with an additional twist that avoids having to check whether either pile is empty in each basic step. We place on the bottom of each pile a *sentinel* card, which contains a special value that we use to simplify our code. Here, we use \infty as the sentinel value, so that whenever a card with \infty is exposed, it cannot be the smaller card unless both piles have their sentinel cards exposed. But once that happens, all the nonsentinel cards have already been placed onto the output pile. Since we know in advance that exactly r - p + 1 cards will be placed onto the output pile, we can stop once we have performed that many basic steps.

** Section 3.1: Asymptotic notation

The order of growth of the running time of an algorithm, defined in Chapter 2, gives a simple characterization of the algorithm's efficiency and also allows us to compare the relative performance of alternative algorithms. Once the input size /n/ becomes large enough, merge sort, with its \Theta(n lg n) worst-case running time, beats insertion sort, whose worst-case running time is \Theta(n^2). Although we can sometimes determine the exact running time of an algorithm, as we did for insertion sort in Chapter 2, the extra precision is *not* usually worth the effort of computing it. For large enough inputs, the multiplicative constants and lower-order terms of an exact running time are dominated by the effects of the input size itself.

When we look at input sizes large enough to make only the order of growth of the running time relevant, we are studying the *asymptotic* efficiency of algorithms. That is, we are concerned with how the running time of an algorithm increases with the size of the input /in the limit/, as the size of the input increases without bound. Usually, an algorithm that is asymptotically more efficient will be the best choice for all but very small inputs.

The notations we use to describe the asymptotic running time of an algorithm are defined in terms of function whose domains are the set of natural numbers \mathbb{N} = {0, 1, 2, ...}. Such notations are convenient for describing the worst-case running-time function /T(n)/, which usually is defined only on integer input sizes. We sometimes find it convenient, however, to /abuse/ asymptotic notation in a variety of ways. For example, we might extend the notation to teh domain of real numbers or, alternatively, restrict it to a subset of the natural numbers. We should make sure, however, to understand the precise meaning of the notation so that when we abuse, we do not /misuse/ it. This section defines the basic asymptotic notations and also introduces some common abuses.

*** Asymptotic notation, functions, and running times
