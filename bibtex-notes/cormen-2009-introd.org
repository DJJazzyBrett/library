#+TITLE: Notes on: Introduction to algorithms by T. H. Cormen (2009)
#+Time-stamp: <2021-06-08 12:09:07 boxx>

- source :: cite:cormen-2009-introd

* TODO Summary

* TODO Comments

* TODO Topics

** Section 2.2: Analyzing algorithms

/Analyzing/ an algorithm has come to mean predicting the resources that the algorithm requires. Occasionally, resources such as memory, communication bandwidth, or computer hardware are of primary concern, but most often it is computational time that we want to measure.

Before we can analyze an algorithm, we must have a model of the implementation technology that we will use, including a model for the resources of that technology and their costs. For most of this book, we shall assume a generic one-processor, *random-access machine (RAM)* model of computation as our implementation technology and understand that our algorithms will be implemented as computer programs. In the RAM model, instructions are executed one after another, with *no* concurrent operations.

Strictly speaking, we should precisely define the instructions of the RAM model and their costs. To do so, however, would be tedious and would yield little insight into algorithm design and analysis. Yet we must be careful *not* to abuse the RAM model. Our guide is how real computers are designed. The RAM model contains instructions commonly found in real computers:

  - arithmetic (such as add, subtract, multiply, divide, remainder, floor, ceiling)
  - data movement (load, store, copy)
  - control (conditional and unconditional branch, subroutine call and return).

Each such instruction takes a constant amount of time.

The data types in the RAM model are integer and flowting point (for storing real numbers). Although we typically do not concern ourselves with precision in this book, in some applications precision is crucial. We also assume a limit on the size of each word of data *(!)*. For example, when working with inputs of size /n/, we typically assume that integers are represented by /c/ lg /n/ bits for some constant /c/ \ge 1 so that each word can hold the value of /n/, enabling us to index the individual input elements, and we restrict /c/ to be a constant so that the word size does *not* grow arbitrarily. (If the word size could grow arbitrarily, we could store huge amounts of data in one word and operate on it all in constant time - clearly an unrealistic scenario.)

Real computers contain instructions not listed above, and such instructions represent a gray area in the RAM model. For example, is exponentiation a constant-time instruction? In the general case, *no*; it takes several instructions to compute x^y when /x/ and /y/ are real numbers. In restricted stiuations, however, exponentiation is a constant-time operation. Many computers have a "shift left" instruction, which in constant time shifts the bits of an integer by /k/ positions to the left. In most computers, shifting the bits of an integer by one position to the left is equivalent to multiplication by 2, so that shifting the bits by /k/ positions to the left is equivalent to multiplication by 2^k. Therefore, such computers can compute 2^k in one constant-time instruction by shifting the integer 1 by /k/ positions to the left, as long as /k/ is no more than the number of bits in a computer word. We will endeavor to avoid such gray areas in the RAM model, but we will treat computation of 2^k as a constant-time operation when /k/ is a small enough positive integer.

In the RAM model, we do *not* attempt to model the memory hierarchy that is common in contemporary computers. That is, we do not model caches or virtual memory. Several computational models attempt to account for memory-hierarchy effects, which are sometimes significant in real programs on real machines. A handful of problems in this book examine memory-hierarchy effects, but for the most part, the analyses in this book will *not* consider them. Models that include the memory hierarchy are quite a bit more complex than the RAM model, and so they can be difficul to work with. Moreover, RAM-model analyses are usually excellent predictors of performance on actual machines.

Analyzing even a simple algorithm in the RAM model can be a challenge. The mathematical tools required may include combinatorics, probability theory, algebraic dexterity, and the ability to find the most significant terms in a formula. Because the behavior of an algorithm may be different for each possible input, we need a means for summarizing that behavior in simple, easily understood formulas.

Even though we typically select only one machine model to analyze a given algorithm, we still face many choices in deciding how to express our analysis. We would like a way that is simple to write and manipulate, shows the important characteristics of an algorithm's resource requirements, and suppresses tedious details.

*** Analysis of insertion sort

The time taken by the ~INSERTION-SORT~ procedure depends on the input; sorting a thousand numbers takes longer than sorting three numbers. Moreover, ~INSERTION-SORT~ can take different amounts of time to sort two input sequences of the same size depending on how nearly sorted they are. In general, the time taken by an algorithm grows with the size of the input, so it is traditional to describe the running time of a program as a function of the size of its input. To do so, we need to define the terms "running time" and "size of input" more carefully.

The best notion for *input size* depends on the problem being studied. For many problems, such as sorting or computing discrete /Fourier Transforms/, the most natural measure is the /number of items in the input/ - for example, the array size /n/ for sorting. For many other problems, such as multiplying two integers, the best measure of input size is the /total number of bits/ needed to represent the input in ordinary binary notation. Sometimes, it is more appropriate to describe the size of the input with two numbers rather than one. For instance, if the input to an algorithm is a graph, the input size can be described by the numbers of vertices and edges in the graph. We shall indicate which input size measure is being used with each problem we study.

The *running time* of an algorithm on a particular input is the number of primitive operations or "steps" executed. It is convenient to define the notion of step so that it is as machine-independent as possible. For the moment, let us adopt the following view. A constant amount of time is required to execute each line of our pseudocode. One line may take a different amount of time than another line, but we shall assume that each execution of the  i^th line takes time c_i, where c_i is a constant. This viewpoint is in keeping with the RAM model, and it also reflects how the pseudocode would be implemented on most actual computers.

We define /T(n)/ to be the running time of a particular algorithm on an input of /n/ values. Typically, as in insertion sort, the running time of an algorithm is fixed for a given input, although in later chapters we shall see some interesting "randomized" algorithms whose behavior can vary even for a fixed input.

*** Worst-case and average-case analysis

In our analysis of insertion sort, we looked at both the best case, in which the input array was already sorted, and the worst case, in which the input array was reverse sorted. For the remainder of this book, though, we shall usually concentrate on finding only the *worst-case running time*, that is, the longest running time for /any/ input of size /n/. We give three (3) reasons for this orientation.

  1. The worst-case running time of an algorithm gives us an upper bound on the running time for any input. Knowing it provides a guarantee that the algorithm will never take any longer. We need not make some educated guess about the running time and hope that it never gets much worse.

  2. For some algorithms, the worst case occurs fairly often. For example, in searching a database for a particular piece of information, the searching algorithm's worst case will often occur when the information is not present in the database. In some applications, searches for absent information may be frequent.

  3. The "average case" is often roughly as bad as the worst case. Suppose that we randomly choose /n/ numbers and apply insertion sort. How long does it take to determine where in subarray A[1..j-1] to insert element A[j]? On average, half the elements in A[1..j-1] are less than A[j], and half the elements are greater. On average, therefore, we check half of the subarray A[1..j-1], and so t_j is about j/2. The resulting average-case running time turns out to be a quadratic function of the input size, just like the worst-case running time.

In some particular cases, we shall be interested in the *average-case* running time of an algorithm; we shall see the technique of *probabilistic analysis* applied to various algorithms throughout this book. The scope of average-case analysis is limited, because it may not be apparent what constitutes an "average" input for a particular problem. Often, we shall assume that all inputs of a given size are equally likely. In practice, this assumption may be violated, but we can sometimes use a *randomized algorithm*, which makes random choices, to allow a probabilistic analysis and yield an *expected* running time.

*** Order of growth

We used some simplifying abstractions to ease our analysis of the ~INSERTION-SORT~ procedure. First, we ignored the actual cost of each statement, using the constants c_i to represent these costs. Then, we observed that even these constants give us more detail than we really need: we expressed the worst-case running time as an^2 + bn + c for some constants a, b, and c that depend on the statement costs c_i. We thus ignored not only the actual statement costs, but also the abstract costs c_i.

We shall now make one more simplifying abstraction: it is the *rate of growth*, or *order of growth*, of the running time that really interests us. We therefore consider only the leading terms of a formula (e.g., an^2), since the lower-order terms are relatively insignificant for large values of /n/. We also ignore the leading term's constant coefficient, since constant factors are less significant that the rate of growth in determining computational efficiency for large inputs.

We usually consider one algorithm to be more efficient that another if its worst-case running time has a lower order of growth *(!)*. Due to constant factors and lower-order terms, an algorithm whose running time has a higher order of growth might take less time for small inputs than an algorithms whose running time has a lower order of growth. But for larger inputs, a \Theta(n^2) algorithm, for example, will run more quickly in the worst case than a \Theta(n^3) algorithm.


** Section 2.3.1: Designing algorthms

We can choose from a wide range of algorithm design techniques. For insertion sort, we used a *incremental approach*: having sorted the subarray A[1..j-1], we inserted the single element A[j] into its proper place, yielding the sorted subarray A[1..j].

In this section, we examine an alternative design approach, known as "divide-and-conquer", which we shall explore in more detail in Chapter 4. We'll use divide-and-conquer to design a sorting algorithm whose worst-case running time is much less than that of insertion sort. One advantage of divide-and-conquer algorithms is that their running times are often easily determine using techniques that we will see in Chapter 4.

*** The divide-and-conquer approach

Many useful algorithms are *recursive* in structure: to solve a given problem, they call themselves recursively one or more times to deal with closely related subproblems. These algorithms typically follow a *divide-and-conquer* approach: they break the problem into several subproblems that are similar to the original problem but smaller in size, solve the subproblems recursively, and then combine these solutions to create a solution to the original problem.

The divide-and-conquer paradigm involves three (3) steps at each level of recursion:

  1. *Divide* the problem into a number of subproblems that are smaller instances of the same problem.

  2. *Conquer* the subproblems by solving them recursively. If the subproblem sizes are small enough, however, just solve the subproblems in a straightforward manner.

  3. *Combine* the solutions to the subproblems into the solution for the original problem.

The *merge sort* algorithm closely follows the divide-and-conquer paradigm. Intuitively, it operates as follows.

  1. *Divide*: Divide the /n/-element sequence to be sorted into two (2) subsequences of n/2 elements each.

  2. *Conquer*: Sort the two (2) subsequences recursively using merge sort.

  3. *Combine*: Merge the two (2) sorted subsequences to produce the sorted answer.

The recursion "bottoms out" when the sequence to be sorted has length 1, in which case there is no work to be done, since every sequence of length 1 is already in sorted order.

The key operation of the merge sort algorithm is the merging of two sorted sequences in the "combine" step. We merge by calling an auxiliary procedure ~MERGE(A,p,q,r)~, where /A/ is an array and /p, q/, and /r/ are indices into the array such that /p/ \le /q/ \lt /r/. The procedue assumes that the subarrays A[p..q] and A[q+1..r] are in sorted order. It *merges* them to form a single sorted subarray that replaces the current subarray A[p..r].

Our ~MERGE~ procedure takes time \Theat(n), where n = r - p + 1 is the total number of elements being merged, and it works as follows. Returning to our card-playing motif, suppose that we have two piles of cards face up on a table. Each pile is sorted, with the smallest cards on top. We wish to merge the two piles into a single sorted output pile, which is to be face down on the table. Our basic step consists of chooseing the smaller of the two cards on top of the face-up piles, removing it from its pile (which exposes a new top card), and placing this card face down onto the output pile. We repeat this step until one input pile is empty, at which time we just take the remaining input pile and place it face down onto the output pile. Computationally, each basic step takes constant time, since we are comparing just the two top cards. Since we perform at most /n/ basic steps, merging takes \Theta(n) time.

The following pseudocode implements the above idea, but *(!)* with an additional twist that avoids having to check whether either pile is empty in each basic step. We place on the bottom of each pile a *sentinel* card, which contains a special value that we use to simplify our code. Here, we use \infty as the sentinel value, so that whenever a card with \infty is exposed, it cannot be the smaller card unless both piles have their sentinel cards exposed. But once that happens, all the nonsentinel cards have already been placed onto the output pile. Since we know in advance that exactly r - p + 1 cards will be placed onto the output pile, we can stop once we have performed that many basic steps.

** Section 3.1: Asymptotic notation

The order of growth of the running time of an algorithm, defined in Chapter 2, gives a simple characterization of the algorithm's efficiency and also allows us to compare the relative performance of alternative algorithms. Once the input size /n/ becomes large enough, merge sort, with its \Theta(n lg n) worst-case running time, beats insertion sort, whose worst-case running time is \Theta(n^2). Although we can sometimes determine the exact running time of an algorithm, as we did for insertion sort in Chapter 2, the extra precision is *not* usually worth the effort of computing it. For large enough inputs, the multiplicative constants and lower-order terms of an exact running time are dominated by the effects of the input size itself.

When we look at input sizes large enough to make only the order of growth of the running time relevant, we are studying the *asymptotic* efficiency of algorithms. That is, we are concerned with how the running time of an algorithm increases with the size of the input /in the limit/, as the size of the input increases without bound. Usually, an algorithm that is asymptotically more efficient will be the best choice for all but very small inputs.

The notations we use to describe the asymptotic running time of an algorithm are defined in terms of function whose domains are the set of natural numbers \mathbb{N} = {0, 1, 2, ...}. Such notations are convenient for describing the worst-case running-time function /T(n)/, which usually is defined only on integer input sizes. We sometimes find it convenient, however, to /abuse/ asymptotic notation in a variety of ways. For example, we might extend the notation to teh domain of real numbers or, alternatively, restrict it to a subset of the natural numbers. We should make sure, however, to understand the precise meaning of the notation so that when we abuse, we do not /misuse/ it. This section defines the basic asymptotic notations and also introduces some common abuses.

*** Asymptotic notation, functions, and running times

We will use asymptotic notation primarily to describe the running times of algorithms, as when we wrote that insertion sort's worst-case running time is \Theta(n^2). Asymptotic notation actually applies to functions, however. Recall that we characterized insertion sort's worst-case running time as (an^2 + bn + c), for some constants a, b, and c. By writing that insertion sort's running time is \Theta(n^2), we abstracted away some details of this function. Because asymptotic notation applies to functions, what we were writing as \Theta(n^2) was the function (an^2 + bn + c), which in that case happened to characterize the worst-case running time of insertion sort.

In this book, the functions to which we apply asymptotic notation will usually characterize the running times of algorithms. But asymptotic notation can apply to functions that characterize some other aspects of algorithms (the amount of space they use, for example), or even to functions that have nothing whatsoever to do with algorithms.

Even when we use asymptotic notation to apply to the running time of an algorithm, we need to understand /which/ running time we mean. Sometimes we are interested in the worst-case running time. Often, however, we wish to characterize the running time no matter what the input. In other words, we often wish to make a blanket statement that covers all inputs, not just the worst case. We shall see asymptotic notations that are well suited to characterizing running times no matter what the input.

*** \Theta-Notation

In Chapter 2, we found that the worst-case running time of insertion sort is T(n) = \Theta(n^2). Let us define what this notation means. For a given function g(n), we denote by \theta(g(n)) the /set of functions/

\Theta(g(n)) = {f(n) : there exist positive constants c_{1}, c_{2}, and n_{0} such that 0 \le c_{1} g(n) \le f(n) \le c_{2} g(n) for all n \ge n_{0}}.

A function ~f(n)~ belongs to the set ~\Theta(g(n))~ if there exist positive constants c_{1} and c_{2} such that it can be "sandwiched" between c_{1}g(n) and c_{2}g(n), for sufficiently large /n/. Because \Theta(g(n)) is a *set*, we could write "f(n) \in \Theta(g(n))" to indicate that f(n) is a member of \Theta(g(n)). Instead, we will usually write "f(n) = \Theta(g(n))" to express the same notion. You might be confused because we abuse equality in this way, but we shall see later in this section that doing so has its advantages *(!)*.

Figure 3.1(a) on page 45 gives an intuitive picture of functions f(n) and g(n), where f(n) = \Theta(g(n)). For all values of /n/ at and to the right of n_0, the value of f(n) lies at or above c_{1}g(n) and at or below c_{2}g(n). In other words, for all n \ge n_{0}, the function f(n) is equal to g(n) within a constant factor. We say that g(n) is an *asymptotically tight bound* for f(n).

The definition of \Theta(g(n)) requires that every member f(n) \in \Theta(g(n)) be *asymptotically nonnegative*, that is, that f(n) be nonnegative whenever /n/ is sufficiently large. (An *asymptotically positive* function is one that is positive for all sufficiently large /n/.) Consequently, the function g(n) itself must be asymptotically nonnegative, or else the set \Theta(g(n)) is empty. We shall therefore assume that every function used within \Theta-notation is asymptotically nonnegative. This assumption holds for the other asymptotic notations defined in this chapter as well.

In Chapter 2, we introduced an informal notion of \Theta-notation that amounted to throwing away lower-order terms and ignoring the leading coefficient of the highest-order term. Intuitively, the lower-order terms of an asymptotically positive function can be ignored in determining asymptotically tight bounds because they are insignificant for large /n/. When /n/ is large, even a tiny fraction of the highest-order term suffices to dominate the lower-order terms. Thus, setting c_{1} to a value that is slightly smaller than the coefficient of the highest-order term and setting c_{2} to a value that is slightly larger permits the inequalities in the definition of \Theta-notation to be satisfied. The coefficient of the highest-order term can likewise be ignored, since it only changes c_{1} and c_{2} by a constant factor equal to the coefficient.

Since any constant is a degree-0 polynomial, we can express any constant function as \Theta(n^0), or \Theta(1). This latter notation is a minor abuse, however, because the expression does *not* indicate what variable is tending to infinity. We shall often use the notation \Theta(1) to mean either a constant or a constant function with respect to some variable.

*** /O/-notation

The \Theta-notation asymptotically bounds a function /from above and below/. When we have only an *asymptotic upper bound*, we use /O/-notation. For a given function g(n), we denote O(g(n)) (pronounced "big-oh of g of n" or sometimes just "oh of g of n") the set of functions

O(g(n)) = { f(n) : there exist positive constants c and n_{0} such that 0 \le f(n) \le cg(n) for all n \ge n_{0 }}.

We use /O/-notation to give an upper bound on a function, to within a constant factor *(!)*. Figure 3.1(b) on page 45 shows the intuition behind /O/-notation. For all values /n/ at and to the right of n_{0}, the value of the function f(n) is on or below cg(n).

We write f(n) = O(g(n)) to indicate that a function f(n) is a member of the set O(g(n)). Note that f(n) = \Theta(g(n)) implies f(n) = O(g(n)), since \Theta-notation is a stronger notation than /O/-notation. Thus, our proof that any quadratic function (an^{2} + bn + c, where a > 0), is in \Theta(n^{2}) also shows that any such quadratic function is in O(n^{2}). What may be more surprising is that when /a/ \gt 0, any  /linear/ function (an + b) is in O(n^{2}).

If you have seen /O/-notation before, you might find it strange that we should write, for example, (n = O(n^{2})). In the literature, we sometimes find /O/-notation informally describing asymptotically tight bounds, that is, what we have defined using \Theta-notation. In this book *(!)*, however, when we write f(n) = O(g(n)), we are merely claiming that some constant multiple of g(n) is an asymptotic upper bound on f(n), with *no* claim about how tight an upper bound it is. Distinguishing asymptotic upper bounds from asymptotically tight bounds is standard in the algorithms literature.

Using /O/-notation, we can often describe the running time of an algorithm merely by inspecting the algorithm's overall structure. Since /O/-notation describes an upper bound, when we use it to bound the worst-case running time of an algorithm, we have a bound on the running time of the algorithm on /every/ input - the blanket statement we discussed earlier. Thus, the O(n^{2}) bound on the worst-case running time of insertion sort also applies to its running time on every input. The \Theta(n^{2}) bound on the worst-case running time of insertion sort, however, does *(!)* *not* *(!)* imply a \Theta(n^{2}) bound on the running time of insertion sort on /every/ input. For example, we saw in Chapter 2 that when the input is already sorted, insertion sort runs in \Theta(n) time.

Technically, it is an abuse to say that the running time of insertion sort is O(n^{2}), since for a given /n/, the actual running time varies, depending on the particular input size /n/. When we say "the running time is O(n^{2})", we mean that there is a function f(n) that is O(n^{2}) such that for any value of /n/, no matter what particular input of size /n/ is chosen, the running time on that input is bounded from above by the value f(n). Equivalently, we mean that the worst-case running time is O(n^{2}).

*** \Omega-notation

Just as /O/-notation provides an asymptotic /upper/ bound on a function, \Omega-notation provides an *asymptotic lower bound*. For a given function g(n), we denote by \Omega(g(n)) (pronounced "big-omega of g of n" or sometimes just "omega of g of n") the set of functions

\Omega(g(n)) = { f(n) : there exist positive constants c and n_{0} such that 0 \le cg(n) \le f(n) for all n \ge n_{0} }

Figure 3.1(c) on page 45 shows the intuition behind \Omega-notation. For all values /n/ at or to the right of n_{0}, the value of f(n) is on or above cg(n).

From the definitions of the asymptotic notations we have seen thus far, it is easy to prove the following important theorem.

#+Theorem 3.1
#+begin_theorem
For any two functions f(n) and g(n), we have f(n) = \Theta(g(n)) if and only if f(n) = O(g(n)) and f(n) = \Omega(g(n)).
#+end_theorem

When we say that the /running time/ (no modifier) of an algorithm is \Omega(g(n)), we mean that /no matter what particular input of size n is chosen for each value of n/, the running time on that input is at least a constant times g(n), for sufficiently large /n/. Equivalently, we are giving a lower bound on the *best-case running time* of an algorithm. For example, the best-case running time of insertion sort is \Omega(n), which implies that the running time of insertion sort is \Omega(n).

The running time of insertion sort therefore belongs to *both* \Omega(n) *and* O(n^{2}), since it falls anywhere between a linear function of /n/ and a quadratic function of /n/. Moreover *(!)*, these bounds are asymptotically as tight as possible: for instance, the running time of insertion sort is *not* \Omega(n^{2}), since there exists an input for which insertion sort runs in \Theta(n) time (e.g., when the input is already sorted). It is not contradictory, however, to say that the /worst-case/ running time of insertion sort is \Omega(n^{2}), since there exists an input that causes the algorithm to take \Omega(n^{2}) time.

*** Asymptotic notation in equations and inequalities

We have already seen how asymptotic notation can be used in mathematical formulas. For example, in introducing /O/-notation, we wrote "n = O(n^{2})." We might also write (2n^2 + 3n + 1 = 2n^2 + \Theta(n)). How do we interpret such formulas?

When the asymptotic notation stands alone (that is, *not* within a larger formula) on the right-hand side of an equation or inequality, as in n = O(n^2), we have already defined the equal sign to mean /set membership/: n \in O(n^2). In general, however, when asymptotic notation appears in a formula, we interpret it as standing for some anonymous function that we do not care to name. For example, the formula (2n^2 + 3n + 1 = 2n^2 + \Theta(n)) means that (2n^2 + 3n + 1 = 2n^2 + f(n)), where f(n) is some function in the set \Theta(n). In this case, we let f(n) = 3n + 1, which indeed is in \Theta(n).

Using asymptotic notation in this manner can help eliminate inessential detail and clutter in an equation. For example, in Chapter 2 we expressed the worst-case running time of merge sort as the recurrence

T(n) = 2T(n/2) + \Theta(n).

If we are interested only in the asymptotic behavior of T(n), there is no point in specifying all the lower-order terms exactly; they are all understood to be included in the anonymous function denoted by the term \Theta(n).

The number of anonymous functions in an expression is understood to be equal to the number of times the asymptotic notation appears *(!)*.

In some cases, asymptotic notation appears on the left-hand side of an equation, as in

2n^2 + \Theta(n) = \Theta(n^2).

We interpret such equations using the following rule:

#+begin_quote
No matter how the anonymous functions are chosen on the left of the equal sign, there is a way to choose the anonymous functions on the right of the equal sign to make the equation valid.
#+end_quote

Thus, our example means that for /any/ function f(n) \in \Theta(n), there is /some/ function g(n) \in \Theta(n^2) such that 2n^2 + f(n) = g(n) for all /n/. In other words, the right-hand side of an equation provides a coarser level of detail thatn the left-hand side.

*** /o/-notation

The asymptotic upper bound provided by /O/-notation may or may not be asymptotically tight. The upper bound 2n^2 = O(n^2) is asymptotically tight, but *(!)* the bound 2n = O(n^2) is not. We use /o/-notation to denote an upper bound that is *not* asymptotically tight. We formally define o(g(n)) (pronounced "little-oh of g of n") as the set

o(g(n)) = { f(n) : for any positive constant c > 0, there exists a constant n_0 \gt 0 such that 0 \le f(n) \lt cg(n) for all n \ge n_0 }.

The definitions of /O/-notation and /o/-notation are similar. The main difference is that in f(n) = )(g(n)), the bound 0 \le f(n) \le cg(n) holds for /some/ constant c \gt 0, but in f(n) = o(g(n)), the bound 0 \le f(n) \lt cg(n) holds for /all/ constants c \gt 0. Intuitively, in /o/-notation, the function f(n) becomes insignificant relative to g(n) as /n/ approaches infinity.

*** \omega-notation

By analogy, \omega-notation is to \Omega-notation as /o/-notation is to /O/-notation. We use \omega-notation to denote a lower bound that is *not* asymptotically tight. One way to define it is by

f(n) \in \omega(g(n)) if and only if g(n) \in o(f(n)).

Formally, however, we define \omega(g(n)) (pronounced "little-omega of g of n") as the set

\omega(g(n)) = { f(n) : for any positive constant c \gt 0, there exists a constant n_0 \gt 0 such that 0 \le cg(n) \lt f(n) for all n \ge n_0 }.

In other words, f(n) becomes arbitrarily large relative to g(n) as /n/ approaches infinity.

*** Comparing functions

Many of the relational properties of real numbers apply to asymptotic comparisons as well. For the following, assume that f(n) and g(n) are asymptotically positive.

**** Transitivity:

- f(n) = \Theta(g(n)) and g(n) = \Theta(h(n)) imply f(n) = \Theta(h(n))
- f(n) = O(g(n)) and g(n) = O(h(n)) imply f(n) = O(h(n))
- f(n) = \Omega(g(n)) and g(n) = \Omega(h(n)) imply f(n) = \Omega(h(n))
- f(n) = o(g(n)) and g(n) = o(h(n)) imply f(n) = o(h(n))
- f(n) = \omega(g(n)) and g(n) = \omega(h(n)) imply f(n) = \omega(h(n))

**** Reflexivity:

- f(n) = \Theta(f(n))
- f(n) = O(f(n))
- f(n) = \Omega(f(n))

**** Symmetry:

- f(n) = \Theta(g(n)) if and only if g(n) = \Theta(f(n))

**** Transpose symmetry:

- f(n) = O(g(n)) if and only if g(n) = \Omega(f(n))
- f(n) = o(g(n)) if and only if g(n) = \omega(f(n))

*** More on comparing functions

Because these properties hold for asymptotic notations, we can draw an analogy between the asymptotic comparison of two functions /f/ and /g/ and the comparison of two real numbers /a/ and /b/:

- f(n) = O(g(n)) is like a \le b
- f(n) = \Omega(g(n)) is like a \ge b
- f(n) = \Theta(g(n)) is like a = b
- f(n) = o(g(n)) is like a \lt b
- f(n) = \omega(g(n)) is like a \gt b

We say that f(n) is *asymptotically smaller* than g(n) if f(n) = o(g(n)), and f(n) is *asymptotically larger* than g(n) if f(n) = \omega(g(n)). One property of real numbers, *(!)* however *(!),* does *not* carry over to asymptotic notation:

**** Trichotomy:

For any two real numbers /a/ and /b/, exactly one of the following must hold:

- a \lt b
- a = b
- a \gt b

Although any two real numbers can be compared, *not* all functions are asymptotically comparable *(!)*. That is, for two functions f(n) and g(n), it may be the case that neither f(n) = O(g(n)) nor f(n) = \Omega(g(n)) holds. For example, we cannot compare the functions f(n) = n and g(n) = n^{1 + sin n} using asymptotic notation, since the value of the exponent in n^{1 + sin n} oscillates between 0 and 2, taking on all values in between.

** Section A.1: Summation formulas and properties

When an algorithm contains an iterative control construct such as a *while* or *for* loop, we can express its running time as the sum of the times spent on each execution of the body of the loop. Section A.2 offers useful techniques for bounding summations. We present formulas in Section A.1 without proof, though proofs for some of them appear in Section A.2 to illustrate the methods of that section. You can find most of the other proofs in any calculus textbook.

*** Linearity

*** Arithmetic series

*** Sums of squares and cubes

*** Geometric series

*** Harmonic series

*** Integrating and differentiating series

*** Telescoping series

*** Products

** Section A.2: Bounding summations

We have many techniques at our disposal for bounding the summations that describe the running time of algorithms. Here are some of the most frequently used methods.

*** Mathematical induction

*** Bounding the terms

*** Splitting summations

*** Approximation by integrals

** Section 4.3: The substitution method for solving recurrences

Now that we have seen how recurrences characterize the running times of divide-and-conquer algorithms, we will learn how to solve recurrences. We start in this section with the /substitution/ method.

The *substitution method* for solving recurrences comprises two steps:

  1. Guess the form of the solution.
  2. Use mathematical induction to find the constants and show that the solution works.

We substitute the guessed solution for the function when applying the inductive hypothesis to smaller values; hence the name "substitution method." This method is powerful, but we must be able to guess the form of the answer in order to apply it.

We can use the substitution method to establish either upper or lower bounds on a recurrence. As an example, let us determine an upper bound on the recurrence

[/ T(n) = 2T(\lfloor n/2 \rfloor) + n /]

which is similar to recurrences (4.3) and (4.4). We guess that the solution is T(n) = O(n lgn). The substitution method requires us to prove that T(n) \le cn lgn for an appropriate choice of the constant /c/ \gt 0. We start by assuming that this bound holds for all positive /m/ \lt /n/, in particular for /m/ = \lfloor n/2 \rfloor, yielding T(\lfloor n/2 \rfloor) \le c \lfloor n/2 \rfloor lg(\lfloor n/2 \rfloor). Substituting into the recurrence yields

\[ T(n) \le 2(c \lfloor n/2 \rfloor lg(\lfloor n/2 \rfloor)) + n \]
\[ T(n) \le cn lg(n/2) + n \]
\[ T(n) = cn lgn - cn lg2 + n \]
\[ T(n) = cn lgn - cn + n \]
\[ T(n) \le cn lg n \]

where the last step holds as long as /c/ \ge 1.

Mathematical induction now requires us to show that our solution holds for the boundary conditions. Typically, we do so by showing that the boundary conditions are suitable as base cases for the inductive proof. For the recurrence (4.19), we must show that we can choose the constant /c/ large enough so that the bound T(n) \le cn lgn works for the boundary conditions as well. This requirement can sometimes lead to problems. Let us assume, for the sake of argument, that T(1) = 1 is the sole boundary condition for the recurrence. Then for n = 1, the bound T(n) \le cn lgn yields T(1) \le 0, which is obviously at odds with T(1) = 1. Consequently, the base case of our inductive proof fails to hold.

We can overcome this obstacle in proving an inductive hypothesis for a specific boundary condition with only a little more effort. In the recurrence (4.19), for example, we take advantage of asymptotic notation requiring us only to prove T(n) \le cn lgn for n \ge n_{0}, where n_{0} is a constant /that we get to choose/. We keep the troublesome boundary condition T(1) = 1, but remove it from consideration in the inductive proof. We do so by first observing that for n \gt 3, the recurrence does *not* depend directly on T(1). Thus, we can replace T(1) by T(2) and T(3) as the base cases in the inductive proof, letting n_{0} = 2. Note that we make a distinction between the base case of the recurrence (n = 1) and the base cases of the inductive proof (n = 2 and n = 3). With T(1) = 1, we derive from the recurrence that T(2) = 4 and T(3) = 5. Now we can complete the inductive proof that T(n) \le cn lgn for some constant c \ge 1 by choosing /c/ large enough so that T(2) \le c2 lgn and T(3) \le c3 lg3. As it turns out, any choice of /c/ \ge 2 suffices for the base case of /n/ = 2 and /n/ = 3 to hold. For most recurrences we shall examine, it is straightforward to extend boundary conditions to make the inductive assumption work for small /n/, and we shall not always explicitly work out the details.

*** Making a good guess

** Section 4.4: The recursion-tree method for solving recurrences

** Section 4.5: The master method for solving recurrences
