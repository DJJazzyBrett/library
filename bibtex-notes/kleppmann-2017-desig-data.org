#+TITLE: Notes on: Kleppmann, M. (2017): Designing Data-Intensive Applications

* PREFACE

** We call an application `data-intensive` if data is its primary challenge - the quantity of data, the complexity of data, or the speed at which it's changing - as opposed to `compute-intensive`, where CPU cycles are the bottleneck.

** The tools and technologies that help data-intensive applications store and process data have been rapidly adapting to these changes. New types of database systems [i.e., NoSQL] have been gaining lots of attention, but message queues, caches, search indexes, frameworks for batch and stream processing, and related technologies are very important too. Many applications use some combination of these.

** The goal of this book is to help you navigate the diverse and fast-changing landscape of technologies for processing and storing data. This book is not a tutorial for one particular tool, nor is it a textbook full of dry theory. Instead, we will look at examples of successful data systems: technologies that form the foundation of many popular applications and that have to meet scalability, performance, and reliability requirements in production every day.

* PART I: FOUNDATIONS OF DATA SYSTEMS

* CHAPTER 1: RELIABLE, SCALABLE, AND MAINTAINABLE APPLICATIONS

** Introduction

*** Many applications today are data-intensive, as opposed to compute-intensive. Raw CPU power is rarely a limiting factor for these applications - bigger problems are usually the amount of data, the complexity of data, and the speed at which it is changing.

*** A data-intensive application is typically built from standard building blocks that provide commonly needed functionality. For example, many applications need to:

**** Store data so that they, or another application, can find it again later [databases]

**** Remember the result of an expensive operation, to speed up the reads [caches]

**** Allow users to search data by different keyword or filter it in various ways [search indexes]

**** Send a message to another process, to be handled asynchronously [stream processing]

**** Periodically crunch a large amount of accumulated data [batch processing]

*** If that sounds painfully obvious, that's just because these data systems are such a successful abstraction: we use them all the time without thinking too much.

** Thinking about data systems

***  When you combine several tools in order to provide a service, the service's interface or application programming interface (API) usually hides those implementation details from clients.

*** There are many factors that may influence the design of a data system, including the skills and experience of the people involved, legacy system dependencies, the timescale for delivery, your organization's tolerance of different kinds of risk, regulatory constraints, etc. Those factors depend very much on the situation.

*** In this book, we focus on three concerns that are important in most software systems:

**** Reliability

***** The system should continue to work correctly [performing the correct function at the desired level of performance] even in the face of adversity [hardware or software faults, and even human errors]

**** Scalability

***** As the system grows [in data volume, traffic, volume, or complexity] there should be reasonable ways of dealing with that growth.

**** Maintainability

***** Over time, many different people will work on the system [engineering and operations, both maintaining current behavior and adapting the system to new use cases], and they should all be able to work on it productively.

*** Reliability

**** The things that can go wrong are called `faults`, and systems that anticipate faults and can cope with them ar called `fault-tolerant` or `resilient`. The former term is slightly misleading: it suggests that we could make a system tolerant of every possible kind of fault, which in reality is not feasible ... so it only makes sense to talk about tolerating `certain types` of faults.

**** Note [!] that a fault is NOT the same as a failure. A fault is usually defined as one component of the system deviating from its spec ... whereas a failure is when the system as a whole stops providing the required service to the user.

**** Counterintuitively, in such fault-tolerant systems, it can make sense to increase [!] the rate of faults by triggering them deliberately - for example, by randomly killing individual processes without warning. Many critical bugs are actually due to poor error handling; by deliberately inducing faults, you ensure that the fault-tolerance machinery is continually exercised and tested, which can increase your confidence that faults will be handled correctly when they occurr naturally.

**** Although we generally prefer tolerating faults over preventing faults [!], there are cases where prevention is better that cure [e.g., because no cure exists]. This is the case with security matters, for example: if an attacker has compromised a system and gained access to sensitive data, that event cannot be undone.

*** Hardware faults

**** Hard disks are reported as having a mean time to failure (MTTF) of about 10 to 50 years. Thus, on a storage cluster with 10,000 disks, we should expect on average one disk to die per day.

**** Our first response is usually to add redundancy to the individual hardware components in order to reduce the failure rate of the system. Disks may be set up in a RAID configuration, servers may have dual power supplies and hot-swappable CPUs, and datacenters may have batteries and diesel generators for backup power.

**** This approach cannot completely prevent hardware problems from causing failures, but it is well understood and can often keep a machine running uninterrupted for years.

**** Until recently, redundancy of hardware components was sufficient for most applications, since it makes total failure of a single machine fairly rare. However, as data volume and applications' computing demands have increased, more applications have begun using larger numbers of machines, which proportionally increase the rate of hardware faults.

**** Hence there is a move towards systems that can tolerate the loss of entire machines, by using software fault-tolerance techniques in preference or in additio to hardware redundancy. Such systems have operational advantages: a single-server system requires planned downtime if you need to reboot the machine [to apply operating system security patches, for example], whereas a system that can tolerate machine failure can be patches one node at a time, without downtime of the entire system [a `rolling upgrade`].

*** Software Errors
